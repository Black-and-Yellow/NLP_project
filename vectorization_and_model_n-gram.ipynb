{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea453cbd",
   "metadata": {},
   "source": [
    "# TF-IDF with N-gram Tokenization and Model Training\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Manual implementation of TF-IDF with **N-gram tokenization** (from scratch)\n",
    "2. Adjustable n-gram range parameter (default: unigrams + bigrams)\n",
    "3. Comparison with unigram-only approach\n",
    "4. Training machine learning models for text classification\n",
    "5. Model evaluation with comprehensive metrics\n",
    "\n",
    "**Key Enhancement:** N-gram tokenization captures word sequences (e.g., \"இலங்கை அரசு\") as features\n",
    "\n",
    "**Dataset:** Tamil news articles with categories and processed text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be12ec",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535cd04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "N-gram tokenization enabled ✓\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"N-gram tokenization enabled ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0bdb86",
   "metadata": {},
   "source": [
    "## 2. Load the Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68228f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (18447, 5)\n",
      "\n",
      "Columns: ['category', 'processed_title', 'cleaned_title', 'tokenized_title', 'title']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>processed_title</th>\n",
       "      <th>cleaned_title</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tamilnadu</td>\n",
       "      <td>மேகதாது விவகாரம் தமிழக கர்நாடகா முதலமைச்சர் நிதின் கட்கரி கடிதம்</td>\n",
       "      <td>மேகதாது விவகாரம் தமிழக கர்நாடகா முதலமைச்சர்களுக்கு நிதின் கட்கரி கடிதம்</td>\n",
       "      <td>மேகதாது விவகாரம் தமிழக கர்நாடகா முதலமைச்சர்களுக்கு நிதின் கட்கரி கடிதம்</td>\n",
       "      <td>மேகதாது விவகாரம்: தமிழக, கர்நாடகா முதலமைச்சர்களுக்கு நிதின் கட்கரி கடிதம்</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>பந்துவீச்சாளர் ஐபிஎல் விளையாடலாமா எதிரெதிர் கருத்தில் தோனி கும்பளே</td>\n",
       "      <td>பந்துவீச்சாளர்கள் ஐபிஎல் விளையாடலாமா எதிரெதிர் கருத்தில் தோனி கும்பளே</td>\n",
       "      <td>பந்துவீச்சாளர்கள் ஐபிஎல் விளையாடலாமா எதிரெதிர் கருத்தில் தோனி கும்பளே</td>\n",
       "      <td>பந்துவீச்சாளர்கள் ஐபிஎல் விளையாடலாமா? - எதிரெதிர் கருத்தில் தோனி-கும்பளே</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tamilnadu</td>\n",
       "      <td>கனமழை எச்சரிக்கை நாளை பள்ளி கல்லூரி விடுமுறை எங்கெல்லாம் தெரியுமா</td>\n",
       "      <td>கனமழை எச்சரிக்கை நாளை பள்ளி கல்லூரிகளுக்கு விடுமுறை எங்கெல்லாம் தெரியுமா</td>\n",
       "      <td>கனமழை எச்சரிக்கை நாளை பள்ளி கல்லூரிகளுக்கு விடுமுறை எங்கெல்லாம் தெரியுமா</td>\n",
       "      <td>கனமழை எச்சரிக்கை | நாளை பள்ளி, கல்லூரிகளுக்கு விடுமுறை.. எங்கெல்லாம் தெரியுமா?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tamilnadu</td>\n",
       "      <td>தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது விஜய் ஆர்பி உதயகுமார் அட்வைஸ்</td>\n",
       "      <td>தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது விஜய்க்கு ஆர்பி உதயகுமார் அட்வைஸ்</td>\n",
       "      <td>தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது விஜய்க்கு ஆர்பி உதயகுமார் அட்வைஸ்</td>\n",
       "      <td>தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது - விஜய்க்கு ஆர்பி உதயகுமார் அட்வைஸ்</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tamilnadu</td>\n",
       "      <td>ஆழ் காற்றழு தாழ்வுப்பகுதி எதிரொலி மாவட்டங் அலர்ட்</td>\n",
       "      <td>ஆழ்ந்த காற்றழுத்த தாழ்வுப்பகுதி எதிரொலி மாவட்டங்களுக்கு அலர்ட்</td>\n",
       "      <td>ஆழ்ந்த காற்றழுத்த தாழ்வுப்பகுதி எதிரொலி மாவட்டங்களுக்கு அலர்ட்</td>\n",
       "      <td>ஆழ்ந்த காற்றழுத்த தாழ்வுப்பகுதி எதிரொலி.. 8 மாவட்டங்களுக்கு RED அலர்ட்..</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category  \\\n",
       "0  tamilnadu   \n",
       "1     sports   \n",
       "2  tamilnadu   \n",
       "3  tamilnadu   \n",
       "4  tamilnadu   \n",
       "\n",
       "                                                       processed_title  \\\n",
       "0     மேகதாது விவகாரம் தமிழக கர்நாடகா முதலமைச்சர் நிதின் கட்கரி கடிதம்   \n",
       "1   பந்துவீச்சாளர் ஐபிஎல் விளையாடலாமா எதிரெதிர் கருத்தில் தோனி கும்பளே   \n",
       "2    கனமழை எச்சரிக்கை நாளை பள்ளி கல்லூரி விடுமுறை எங்கெல்லாம் தெரியுமா   \n",
       "3  தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது விஜய் ஆர்பி உதயகுமார் அட்வைஸ்   \n",
       "4                    ஆழ் காற்றழு தாழ்வுப்பகுதி எதிரொலி மாவட்டங் அலர்ட்   \n",
       "\n",
       "                                                              cleaned_title  \\\n",
       "0   மேகதாது விவகாரம் தமிழக கர்நாடகா முதலமைச்சர்களுக்கு நிதின் கட்கரி கடிதம்   \n",
       "1     பந்துவீச்சாளர்கள் ஐபிஎல் விளையாடலாமா எதிரெதிர் கருத்தில் தோனி கும்பளே   \n",
       "2  கனமழை எச்சரிக்கை நாளை பள்ளி கல்லூரிகளுக்கு விடுமுறை எங்கெல்லாம் தெரியுமா   \n",
       "3   தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது விஜய்க்கு ஆர்பி உதயகுமார் அட்வைஸ்   \n",
       "4            ஆழ்ந்த காற்றழுத்த தாழ்வுப்பகுதி எதிரொலி மாவட்டங்களுக்கு அலர்ட்   \n",
       "\n",
       "                                                            tokenized_title  \\\n",
       "0   மேகதாது விவகாரம் தமிழக கர்நாடகா முதலமைச்சர்களுக்கு நிதின் கட்கரி கடிதம்   \n",
       "1     பந்துவீச்சாளர்கள் ஐபிஎல் விளையாடலாமா எதிரெதிர் கருத்தில் தோனி கும்பளே   \n",
       "2  கனமழை எச்சரிக்கை நாளை பள்ளி கல்லூரிகளுக்கு விடுமுறை எங்கெல்லாம் தெரியுமா   \n",
       "3   தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது விஜய்க்கு ஆர்பி உதயகுமார் அட்வைஸ்   \n",
       "4            ஆழ்ந்த காற்றழுத்த தாழ்வுப்பகுதி எதிரொலி மாவட்டங்களுக்கு அலர்ட்   \n",
       "\n",
       "                                                                            title  \n",
       "0       மேகதாது விவகாரம்: தமிழக, கர்நாடகா முதலமைச்சர்களுக்கு நிதின் கட்கரி கடிதம்  \n",
       "1        பந்துவீச்சாளர்கள் ஐபிஎல் விளையாடலாமா? - எதிரெதிர் கருத்தில் தோனி-கும்பளே  \n",
       "2  கனமழை எச்சரிக்கை | நாளை பள்ளி, கல்லூரிகளுக்கு விடுமுறை.. எங்கெல்லாம் தெரியுமா?  \n",
       "3       தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது - விஜய்க்கு ஆர்பி உதயகுமார் அட்வைஸ்  \n",
       "4        ஆழ்ந்த காற்றழுத்த தாழ்வுப்பகுதி எதிரொலி.. 8 மாவட்டங்களுக்கு RED அலர்ட்..  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the processed data from previous notebook\n",
    "df = pd.read_csv('output/processed_data.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3603d0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category distribution:\n",
      "category\n",
      "india                                     2191\n",
      "world                                     1893\n",
      "cinema                                    1811\n",
      "sports                                    1748\n",
      "crime                                     1627\n",
      "tamilnadu                                 1589\n",
      "business                                  1304\n",
      "trending                                  1271\n",
      "technology                                1225\n",
      "features                                  1196\n",
      "health                                     841\n",
      "environment                                639\n",
      "agriculture                                613\n",
      "spiritual                                  245\n",
      "lifestyle                                  103\n",
      "motor                                       45\n",
      "coronavirus                                 38\n",
      "ampstories                                  30\n",
      "women                                       20\n",
      "employment-news-in-tamil-latest-update      18\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total samples: 18447\n",
      "Missing values:\n",
      "category           0\n",
      "processed_title    0\n",
      "cleaned_title      0\n",
      "tokenized_title    0\n",
      "title              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check data distribution\n",
    "print(\"Category distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "print(f\"\\nTotal samples: {len(df)}\")\n",
    "print(f\"Missing values:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8b3cf4",
   "metadata": {},
   "source": [
    "## 3. Prepare Text Data for TF-IDF\n",
    "\n",
    "We'll use the `cleaned_title` column which contains preprocessed Tamil text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a510e9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 18447\n",
      "Total labels: 18447\n",
      "\n",
      "Sample document: மேகதாது விவகாரம் தமிழக கர்நாடகா முதலமைச்சர்களுக்கு நிதின் கட்கரி கடிதம்\n",
      "Sample label: tamilnadu\n"
     ]
    }
   ],
   "source": [
    "# Select the text column and target variable\n",
    "documents = df['cleaned_title'].fillna('').tolist()\n",
    "labels = df['category'].tolist()\n",
    "\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "print(f\"Total labels: {len(labels)}\")\n",
    "print(f\"\\nSample document: {documents[0]}\")\n",
    "print(f\"Sample label: {labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f64270c",
   "metadata": {},
   "source": [
    "## 4. TF-IDF Implementation from Scratch with N-grams\n",
    "\n",
    "### Step 1: N-gram Tokenization\n",
    "\n",
    "**Key Enhancement:** Instead of splitting only into single words (unigrams), we now generate word n-grams based on `ngram_range`.\n",
    "\n",
    "- **Unigrams (1-gram):** [\"இலங்கை\", \"அரசு\", \"தீர்மானம்\"]\n",
    "- **Bigrams (2-gram):** [\"இலங்கை அரசு\", \"அரசு தீர்மானம்\"]\n",
    "- **ngram_range=(1,2):** Both unigrams and bigrams combined\n",
    "\n",
    "This captures context and word sequences as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1efe2cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens (unigrams only):\n",
      "['மேகதாது', 'விவகாரம்', 'தமிழக', 'கர்நாடகா', 'முதலமைச்சர்களுக்கு', 'நிதின்', 'கட்கரி', 'கடிதம்']\n",
      "\n",
      "N-grams with ngram_range=(1, 2):\n",
      "['மேகதாது', 'விவகாரம்', 'தமிழக', 'கர்நாடகா', 'முதலமைச்சர்களுக்கு', 'நிதின்', 'கட்கரி', 'கடிதம்', 'மேகதாது விவகாரம்', 'விவகாரம் தமிழக', 'தமிழக கர்நாடகா', 'கர்நாடகா முதலமைச்சர்களுக்கு', 'முதலமைச்சர்களுக்கு நிதின்', 'நிதின் கட்கரி', 'கட்கரி கடிதம்']\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(tokens, ngram_range=(1, 2)):\n",
    "    \"\"\"\n",
    "    Generate n-grams from a list of tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of words (tokens)\n",
    "        ngram_range: Tuple (min_n, max_n) specifying the range of n-grams\n",
    "    \n",
    "    Returns:\n",
    "        List of n-gram strings\n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    min_n, max_n = ngram_range\n",
    "    \n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = ' '.join(tokens[i:i+n])\n",
    "            ngrams.append(ngram)\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "def tokenize_with_ngrams(text, ngram_range=(1, 2)):\n",
    "    \"\"\"\n",
    "    Tokenize text and generate n-grams.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    return generate_ngrams(tokens, ngram_range)\n",
    "\n",
    "# Example of n-gram tokenization\n",
    "sample_text = documents[0] if documents[0] else \"இலங்கை அரசு தீர்மானம்\"\n",
    "sample_tokens = sample_text.split()\n",
    "\n",
    "print(\"Original tokens (unigrams only):\")\n",
    "print(sample_tokens[:10])\n",
    "\n",
    "print(\"\\nN-grams with ngram_range=(1, 2):\")\n",
    "ngrams_sample = generate_ngrams(sample_tokens, ngram_range=(1, 2))\n",
    "print(ngrams_sample[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "048f453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 18447 documents with ngram_range=(1, 2)\n",
      "Sample tokenized document length: 15 n-grams\n"
     ]
    }
   ],
   "source": [
    "# Configurable n-gram range parameter\n",
    "NGRAM_RANGE = (1, 2)  # Default: unigrams + bigrams\n",
    "\n",
    "# Apply n-gram tokenization to all documents\n",
    "tokenized_docs = [tokenize_with_ngrams(doc, ngram_range=NGRAM_RANGE) for doc in documents]\n",
    "print(f\"Tokenized {len(tokenized_docs)} documents with ngram_range={NGRAM_RANGE}\")\n",
    "print(f\"Sample tokenized document length: {len(tokenized_docs[0])} n-grams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8558e",
   "metadata": {},
   "source": [
    "### Step 2: Build Vocabulary with N-grams\n",
    "\n",
    "Create a vocabulary including all unique n-grams in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3566aac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size with ngram_range=(1, 2): 144438 unique n-grams\n",
      "\n",
      "Sample vocabulary entries:\n",
      "['°', '° சூர்யகுமார்', 'ñ', 'ñ என்பது', 'ஃபகத்', 'ஃபகத் ஃபாசிலை', 'ஃபகத் ஃபாசில்', 'ஃபகார்', 'ஃபகார் ஜமான்', 'ஃபட்னாவிஸ்']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "for doc in tokenized_docs:\n",
    "    vocabulary.update(doc)\n",
    "\n",
    "vocabulary = sorted(list(vocabulary))\n",
    "word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "print(f\"Vocabulary size with ngram_range={NGRAM_RANGE}: {len(vocabulary)} unique n-grams\")\n",
    "print(f\"\\nSample vocabulary entries:\")\n",
    "print(vocabulary[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2a641a",
   "metadata": {},
   "source": [
    "### Step 3: Compute Term Frequency (TF) for N-grams\n",
    "\n",
    "**Term Frequency (TF)** measures how frequently an n-gram appears in a document.\n",
    "\n",
    "Formula: `TF(t, d) = (Number of times n-gram t appears in document d) / (Total number of n-grams in document d)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa306544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency computed for n-grams\n",
      "Sample TF dict entries: [('மேகதாது', 0.06666666666666667), ('விவகாரம்', 0.06666666666666667), ('தமிழக', 0.06666666666666667), ('கர்நாடகா', 0.06666666666666667), ('முதலமைச்சர்களுக்கு', 0.06666666666666667)]\n"
     ]
    }
   ],
   "source": [
    "def compute_tf(tokenized_doc):\n",
    "    \"\"\"Compute term frequency for n-grams: TF(t) = count(t) / total_ngrams\"\"\"\n",
    "    tf_dict = {}\n",
    "    doc_length = len(tokenized_doc)\n",
    "    if doc_length == 0:\n",
    "        return tf_dict\n",
    "    term_counts = Counter(tokenized_doc)\n",
    "    for term, count in term_counts.items():\n",
    "        tf_dict[term] = count / doc_length\n",
    "    return tf_dict\n",
    "\n",
    "tf_docs = [compute_tf(doc) for doc in tokenized_docs]\n",
    "print(\"Term Frequency computed for n-grams\")\n",
    "print(f\"Sample TF dict entries: {list(tf_docs[0].items())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a81603e",
   "metadata": {},
   "source": [
    "### Step 4: Compute Inverse Document Frequency (IDF) for N-grams\n",
    "\n",
    "**Inverse Document Frequency (IDF)** measures how important an n-gram is across the entire corpus.\n",
    "\n",
    "Formula: `IDF(t) = log(Total number of documents / Number of documents containing n-gram t)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d14b2256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing IDF for n-grams...\n",
      "IDF computed for 144438 n-grams\n",
      "Sample IDF values: [('°', 9.129509854061634), ('° சூர்யகுமார்', 9.129509854061634), ('ñ', 9.129509854061634), ('ñ என்பது', 9.129509854061634), ('ஃபகத்', 8.436362673501689)]\n",
      "IDF computed for 144438 n-grams\n",
      "Sample IDF values: [('°', 9.129509854061634), ('° சூர்யகுமார்', 9.129509854061634), ('ñ', 9.129509854061634), ('ñ என்பது', 9.129509854061634), ('ஃபகத்', 8.436362673501689)]\n"
     ]
    }
   ],
   "source": [
    "def compute_idf(tokenized_docs, vocabulary):\n",
    "    \"\"\"Compute IDF for n-grams: IDF(t) = log(N / df(t))\"\"\"\n",
    "    N = len(tokenized_docs)\n",
    "    idf_dict = {}\n",
    "    for word in vocabulary:\n",
    "        doc_count = sum(1 for doc in tokenized_docs if word in doc)\n",
    "        idf_dict[word] = math.log(N / (doc_count + 1))\n",
    "    return idf_dict\n",
    "\n",
    "print(\"Computing IDF for n-grams...\")\n",
    "idf_dict = compute_idf(tokenized_docs, vocabulary)\n",
    "print(f\"IDF computed for {len(idf_dict)} n-grams\")\n",
    "print(f\"Sample IDF values: {list(idf_dict.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23794280",
   "metadata": {},
   "source": [
    "### Step 5: Compute TF-IDF Weights for N-grams\n",
    "\n",
    "**TF-IDF** combines both TF and IDF to get the final weight for each n-gram in each document.\n",
    "\n",
    "Formula: `TF-IDF(t, d) = TF(t, d) × IDF(t)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27df7c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF weights computed for n-grams\n"
     ]
    }
   ],
   "source": [
    "def compute_tfidf(tf_dict, idf_dict):\n",
    "    \"\"\"Compute TF-IDF for n-grams: TF-IDF(t, d) = TF(t, d) × IDF(t)\"\"\"\n",
    "    tfidf_dict = {}\n",
    "    for term, tf_value in tf_dict.items():\n",
    "        tfidf_dict[term] = tf_value * idf_dict.get(term, 0)\n",
    "    return tfidf_dict\n",
    "\n",
    "tfidf_docs = [compute_tfidf(tf, idf_dict) for tf in tf_docs]\n",
    "print(\"TF-IDF weights computed for n-grams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa8f495",
   "metadata": {},
   "source": [
    "### Step 6: Create TF-IDF Matrix with N-grams\n",
    "\n",
    "Convert the TF-IDF dictionaries into a matrix representation where:\n",
    "- Rows represent documents\n",
    "- Columns represent n-grams in vocabulary\n",
    "- Values are TF-IDF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3beb6f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF matrix with n-grams...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 19.9 GiB for an array with shape (18447, 144438) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tfidf_matrix\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating TF-IDF matrix with n-grams...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m tfidf_matrix_custom = \u001b[43mcreate_tfidf_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword2idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTF-IDF Matrix: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtfidf_matrix_custom.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - Documents: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtfidf_matrix_custom.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mcreate_tfidf_matrix\u001b[39m\u001b[34m(tfidf_docs, vocabulary, word2idx)\u001b[39m\n\u001b[32m      3\u001b[39m n_docs = \u001b[38;5;28mlen\u001b[39m(tfidf_docs)\n\u001b[32m      4\u001b[39m n_vocab = \u001b[38;5;28mlen\u001b[39m(vocabulary)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tfidf_matrix = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_vocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc_idx, tfidf_dict \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tfidf_docs):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m term, tfidf_value \u001b[38;5;129;01min\u001b[39;00m tfidf_dict.items():\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 19.9 GiB for an array with shape (18447, 144438) and data type float64"
     ]
    }
   ],
   "source": [
    "def create_tfidf_matrix(tfidf_docs, vocabulary, word2idx):\n",
    "    \"\"\"Create TF-IDF matrix: (n_documents, n_vocabulary)\"\"\"\n",
    "    n_docs = len(tfidf_docs)\n",
    "    n_vocab = len(vocabulary)\n",
    "    tfidf_matrix = np.zeros((n_docs, n_vocab))\n",
    "    \n",
    "    for doc_idx, tfidf_dict in enumerate(tfidf_docs):\n",
    "        for term, tfidf_value in tfidf_dict.items():\n",
    "            if term in word2idx:\n",
    "                term_idx = word2idx[term]\n",
    "                tfidf_matrix[doc_idx, term_idx] = tfidf_value\n",
    "    \n",
    "    return tfidf_matrix\n",
    "\n",
    "print(\"Creating TF-IDF matrix with n-grams...\")\n",
    "tfidf_matrix_custom = create_tfidf_matrix(tfidf_docs, vocabulary, word2idx)\n",
    "print(f\"TF-IDF Matrix: {tfidf_matrix_custom.shape}\")\n",
    "print(f\"  - Documents: {tfidf_matrix_custom.shape[0]}\")\n",
    "print(f\"  - N-gram features: {tfidf_matrix_custom.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a57c10b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_matrix_custom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m plt.imshow(\u001b[43mtfidf_matrix_custom\u001b[49m[:\u001b[32m10\u001b[39m, :\u001b[32m20\u001b[39m], aspect=\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m, cmap=\u001b[33m'\u001b[39m\u001b[33mYlOrRd\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m plt.colorbar(label=\u001b[33m'\u001b[39m\u001b[33mTF-IDF Weight\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33mN-gram Feature Index\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tfidf_matrix_custom' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(tfidf_matrix_custom[:10, :20], aspect='auto', cmap='YlOrRd')\n",
    "plt.colorbar(label='TF-IDF Weight')\n",
    "plt.xlabel('N-gram Feature Index')\n",
    "plt.ylabel('Document Index')\n",
    "plt.title(f'TF-IDF Matrix Visualization (with N-grams, range={NGRAM_RANGE})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9558b4",
   "metadata": {},
   "source": [
    "## 5. TF-IDF Matrix Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e964bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display TF-IDF matrix statistics\n",
    "print(f\"TF-IDF Matrix Statistics (with N-grams {NGRAM_RANGE}):\")\n",
    "print(f\"  Shape: {tfidf_matrix_custom.shape}\")\n",
    "print(f\"  Mean: {tfidf_matrix_custom.mean():.6f}\")\n",
    "print(f\"  Max: {tfidf_matrix_custom.max():.6f}\")\n",
    "print(f\"  Sparsity: {(tfidf_matrix_custom == 0).sum() / tfidf_matrix_custom.size * 100:.2f}%\")\n",
    "print(f\"\\nNote: N-gram features = {tfidf_matrix_custom.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1e0e2",
   "metadata": {},
   "source": [
    "## 6. Compare Unigrams vs N-grams (Feature Count)\n",
    "\n",
    "Before training models, let's compare the vocabulary size difference between unigrams and n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a5c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute unigram-only vocabulary for comparison\n",
    "tokenized_unigrams = [tokenize_with_ngrams(doc, ngram_range=(1, 1)) for doc in documents]\n",
    "vocab_unigrams = set()\n",
    "for doc in tokenized_unigrams:\n",
    "    vocab_unigrams.update(doc)\n",
    "\n",
    "print(\"VOCABULARY SIZE COMPARISON:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Unigrams only (1,1):      {len(vocab_unigrams):,} features\")\n",
    "print(f\"N-grams {NGRAM_RANGE}:         {len(vocabulary):,} features\")\n",
    "print(f\"Increase:                 {len(vocabulary) - len(vocab_unigrams):,} features\")\n",
    "print(f\"Percentage increase:      {((len(vocabulary) / len(vocab_unigrams)) - 1) * 100:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e87585",
   "metadata": {},
   "source": [
    "## 7. Train Machine Learning Models\n",
    "\n",
    "Training three classification models:\n",
    "1. **Naive Bayes** - Probabilistic model\n",
    "2. **Linear SVM** - Support Vector Machine with linear kernel\n",
    "3. **Logistic Regression** - Linear model with multinomial classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a40a49c",
   "metadata": {},
   "source": [
    "### 7.1 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad417e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_train_pred = nb_model.predict(X_train)\n",
    "nb_test_pred = nb_model.predict(X_test)\n",
    "print(\"Naive Bayes training completed with n-gram features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd4f19",
   "metadata": {},
   "source": [
    "### 7.2 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2530a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = LinearSVC(C=1.0, random_state=42, max_iter=1000)\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_train_pred = svm_model.predict(X_train)\n",
    "svm_test_pred = svm_model.predict(X_test)\n",
    "print(\"Linear SVM training completed with n-gram features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54000369",
   "metadata": {},
   "source": [
    "### 7.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba7373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial')\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_train_pred = lr_model.predict(X_train)\n",
    "lr_test_pred = lr_model.predict(X_test)\n",
    "print(\"Logistic Regression training completed with n-gram features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c51489",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "\n",
    "Evaluate all models using multiple metrics:\n",
    "- **Accuracy**: Overall correctness\n",
    "- **Precision**: How many selected items are relevant\n",
    "- **Recall**: How many relevant items are selected\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **Confusion Matrix**: Detailed breakdown of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9986032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with multiple metrics.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name} - {dataset_name} Set\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "    print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "    print(f\"F1-Score:  {f1:.4f} ({f1*100:.2f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abd928d",
   "metadata": {},
   "source": [
    "### 8.1 Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d632d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Naive Bayes\n",
    "nb_train_metrics = evaluate_model(y_train, nb_train_pred, \"Naive Bayes (N-gram)\", \"Training\")\n",
    "nb_test_metrics = evaluate_model(y_test, nb_test_pred, \"Naive Bayes (N-gram)\", \"Test\")\n",
    "\n",
    "# Evaluate Linear SVM\n",
    "svm_train_metrics = evaluate_model(y_train, svm_train_pred, \"Linear SVM (N-gram)\", \"Training\")\n",
    "svm_test_metrics = evaluate_model(y_test, svm_test_pred, \"Linear SVM (N-gram)\", \"Test\")\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "lr_train_metrics = evaluate_model(y_train, lr_train_pred, \"Logistic Regression (N-gram)\", \"Training\")\n",
    "lr_test_metrics = evaluate_model(y_test, lr_test_pred, \"Logistic Regression (N-gram)\", \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f391c",
   "metadata": {},
   "source": [
    "### 8.2 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f1ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(list(set(y_test)))\n",
    "\n",
    "# Create confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Naive Bayes\n",
    "nb_cm = confusion_matrix(y_test, nb_test_pred)\n",
    "sns.heatmap(nb_cm, annot=True, fmt='d', cmap='Greens', ax=axes[0],\n",
    "            xticklabels=classes, yticklabels=classes, cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title('Naive Bayes (N-gram)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "\n",
    "# Linear SVM\n",
    "svm_cm = confusion_matrix(y_test, svm_test_pred)\n",
    "sns.heatmap(svm_cm, annot=True, fmt='d', cmap='Oranges', ax=axes[1],\n",
    "            xticklabels=classes, yticklabels=classes, cbar_kws={'label': 'Count'})\n",
    "axes[1].set_title('Linear SVM (N-gram)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "\n",
    "# Logistic Regression\n",
    "lr_cm = confusion_matrix(y_test, lr_test_pred)\n",
    "sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', ax=axes[2],\n",
    "            xticklabels=classes, yticklabels=classes, cbar_kws={'label': 'Count'})\n",
    "axes[2].set_title('Logistic Regression (N-gram)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Predicted')\n",
    "axes[2].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaa7107",
   "metadata": {},
   "source": [
    "## 9. Model Comparison with N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e7beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Naive Bayes', 'Linear SVM', 'Logistic Regression'],\n",
    "    'Train Accuracy': [nb_train_metrics['accuracy'], svm_train_metrics['accuracy'], lr_train_metrics['accuracy']],\n",
    "    'Test Accuracy': [nb_test_metrics['accuracy'], svm_test_metrics['accuracy'], lr_test_metrics['accuracy']],\n",
    "    'Test Precision': [nb_test_metrics['precision'], svm_test_metrics['precision'], lr_test_metrics['precision']],\n",
    "    'Test Recall': [nb_test_metrics['recall'], svm_test_metrics['recall'], lr_test_metrics['recall']],\n",
    "    'Test F1-Score': [nb_test_metrics['f1'], svm_test_metrics['f1'], lr_test_metrics['f1']],\n",
    "    'N-gram Range': [str(NGRAM_RANGE)] * 3,\n",
    "    'Feature Count': [len(vocabulary)] * 3\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY (WITH N-GRAMS)\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719039d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "metrics = ['Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1-Score']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width, \n",
    "                [nb_test_metrics['accuracy'], nb_test_metrics['precision'], \n",
    "                 nb_test_metrics['recall'], nb_test_metrics['f1']], \n",
    "                width, label='Naive Bayes', color='lightgreen')\n",
    "rects2 = ax.bar(x, \n",
    "                [svm_test_metrics['accuracy'], svm_test_metrics['precision'], \n",
    "                 svm_test_metrics['recall'], svm_test_metrics['f1']], \n",
    "                width, label='Linear SVM', color='orange')\n",
    "rects3 = ax.bar(x + width, \n",
    "                [lr_test_metrics['accuracy'], lr_test_metrics['precision'], \n",
    "                 lr_test_metrics['recall'], lr_test_metrics['f1']], \n",
    "                width, label='Logistic Regression', color='skyblue')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title(f'Model Performance Comparison (N-gram range={NGRAM_RANGE})', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6e40a",
   "metadata": {},
   "source": [
    "## 10. Unigrams vs N-grams Comparison\n",
    "\n",
    "**Key Analysis:** Compare feature count and model accuracy between unigram-only and n-gram approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79256201",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"UNIGRAMS VS N-GRAMS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display vocabulary comparison from earlier\n",
    "print(f\"\\nFeature Count Comparison:\")\n",
    "print(f\"  Unigrams only (1,1):    {unigram_vocab_size:,} features\")\n",
    "print(f\"  N-grams (1,2):          {len(vocabulary):,} features\")\n",
    "print(f\"  Increase:               {len(vocabulary) - unigram_vocab_size:,} features ({((len(vocabulary) - unigram_vocab_size) / unigram_vocab_size * 100):.1f}% more)\")\n",
    "\n",
    "print(f\"\\nN-gram Range: {NGRAM_RANGE}\")\n",
    "print(f\"  - Captures word sequences\")\n",
    "print(f\"  - Better context understanding\")\n",
    "print(f\"  - More discriminative features\")\n",
    "\n",
    "print(f\"\\nBest Model Performance (N-gram {NGRAM_RANGE}):\")\n",
    "best_f1 = max(nb_test_metrics['f1'], svm_test_metrics['f1'], lr_test_metrics['f1'])\n",
    "if best_f1 == nb_test_metrics['f1']:\n",
    "    print(f\"  Model: Naive Bayes\")\n",
    "elif best_f1 == svm_test_metrics['f1']:\n",
    "    print(f\"  Model: Linear SVM\")\n",
    "else:\n",
    "    print(f\"  Model: Logistic Regression\")\n",
    "print(f\"  F1-Score: {best_f1:.4f}\")\n",
    "print(f\"  Accuracy: {max(nb_test_metrics['accuracy'], svm_test_metrics['accuracy'], lr_test_metrics['accuracy']):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature count comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "categories = ['Unigrams\\n(1,1)', f'N-grams\\n{NGRAM_RANGE}']\n",
    "feature_counts = [unigram_vocab_size, len(vocabulary)]\n",
    "colors = ['skyblue', 'coral']\n",
    "\n",
    "bars = ax.bar(categories, feature_counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Number of Features', fontsize=12)\n",
    "ax.set_title('Feature Count: Unigrams vs N-grams', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height):,}',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21b33a",
   "metadata": {},
   "source": [
    "## 11. Save Models and Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c028f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "# Determine best model\n",
    "best_model_name = 'Naive Bayes'\n",
    "best_f1 = nb_test_metrics['f1']\n",
    "best_model = nb_model\n",
    "\n",
    "if svm_test_metrics['f1'] > best_f1:\n",
    "    best_model_name = 'Linear SVM'\n",
    "    best_f1 = svm_test_metrics['f1']\n",
    "    best_model = svm_model\n",
    "    \n",
    "if lr_test_metrics['f1'] > best_f1:\n",
    "    best_model_name = 'Logistic Regression'\n",
    "    best_f1 = lr_test_metrics['f1']\n",
    "    best_model = lr_model\n",
    "\n",
    "# Save all trained models with n-gram suffix\n",
    "with open('models/category_naive_bayes_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(nb_model, f)\n",
    "    \n",
    "with open('models/category_svm_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_model, f)\n",
    "    \n",
    "with open('models/category_logistic_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "\n",
    "# Save the best model\n",
    "with open('models/category_model_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Save vectorizer components (vocabulary, word2idx, IDF, and ngram_range)\n",
    "vectorizer_data = {\n",
    "    'vocabulary': vocabulary,\n",
    "    'word2idx': word2idx,\n",
    "    'idf_dict': idf_dict,\n",
    "    'ngram_range': NGRAM_RANGE\n",
    "}\n",
    "with open('models/category_vectorizer_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer_data, f)\n",
    "\n",
    "# Save model comparison\n",
    "comparison_df.to_csv('output/model_comparison_ngram.csv', index=False)\n",
    "\n",
    "# Save evaluation reports\n",
    "import json\n",
    "\n",
    "reports = {\n",
    "    'naive_bayes': {\n",
    "        'train_metrics': nb_train_metrics,\n",
    "        'test_metrics': nb_test_metrics,\n",
    "        'classification_report': classification_report(y_test, nb_test_pred, output_dict=True, zero_division=0)\n",
    "    },\n",
    "    'svm': {\n",
    "        'train_metrics': svm_train_metrics,\n",
    "        'test_metrics': svm_test_metrics,\n",
    "        'classification_report': classification_report(y_test, svm_test_pred, output_dict=True, zero_division=0)\n",
    "    },\n",
    "    'logistic': {\n",
    "        'train_metrics': lr_train_metrics,\n",
    "        'test_metrics': lr_test_metrics,\n",
    "        'classification_report': classification_report(y_test, lr_test_pred, output_dict=True, zero_division=0)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('reports/category_naive_bayes_ngram_report.json', 'w') as f:\n",
    "    json.dump(reports['naive_bayes'], f, indent=2)\n",
    "    \n",
    "with open('reports/category_svm_ngram_report.json', 'w') as f:\n",
    "    json.dump(reports['svm'], f, indent=2)\n",
    "    \n",
    "with open('reports/category_logistic_ngram_report.json', 'w') as f:\n",
    "    json.dump(reports['logistic'], f, indent=2)\n",
    "\n",
    "print(\"✓ All models saved to models/ directory (with _ngram suffix)\")\n",
    "print(\"✓ Vectorizer saved to models/category_vectorizer_ngram.pkl\")\n",
    "print(\"✓ Evaluation reports saved to reports/ directory\")\n",
    "print(f\"✓ Best model ({best_model_name}) saved as models/category_model_ngram.pkl\")\n",
    "print(f\"✓ N-gram range: {NGRAM_RANGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eae12c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SENTIMENT CLASSIFICATION WITH N-GRAMS\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Load Sentiment Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = pd.read_csv('output/processed_sentiment_data.csv')\n",
    "\n",
    "print(f\"Sentiment Dataset shape: {df_sentiment.shape}\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df_sentiment['sentiment'].value_counts())\n",
    "\n",
    "sentiment_documents = df_sentiment['tokenized_title'].fillna('').tolist()\n",
    "sentiment_labels = df_sentiment['sentiment'].tolist()\n",
    "\n",
    "print(f\"\\nTotal sentiment documents: {len(sentiment_documents)}\")\n",
    "print(f\"Sample: {sentiment_documents[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035104b",
   "metadata": {},
   "source": [
    "## 13. TF-IDF for Sentiment with N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7d51f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply n-gram tokenization to sentiment documents\n",
    "sentiment_tokenized_docs = [tokenize_with_ngrams(doc, ngram_range=NGRAM_RANGE) for doc in sentiment_documents]\n",
    "print(f\"Tokenized {len(sentiment_tokenized_docs)} sentiment documents with ngram_range={NGRAM_RANGE}\")\n",
    "\n",
    "# Build vocabulary for sentiment with n-grams\n",
    "sentiment_vocabulary = set()\n",
    "for doc in sentiment_tokenized_docs:\n",
    "    sentiment_vocabulary.update(doc)\n",
    "sentiment_vocabulary = sorted(list(sentiment_vocabulary))\n",
    "sentiment_word2idx = {word: idx for idx, word in enumerate(sentiment_vocabulary)}\n",
    "print(f\"Sentiment vocabulary size with n-grams: {len(sentiment_vocabulary)} unique n-grams\")\n",
    "\n",
    "# Compute TF for sentiment\n",
    "sentiment_tf_docs = [compute_tf(doc) for doc in sentiment_tokenized_docs]\n",
    "print(\"Sentiment TF computed\")\n",
    "\n",
    "# Compute IDF for sentiment\n",
    "print(\"Computing sentiment IDF with n-grams...\")\n",
    "sentiment_idf_dict = compute_idf(sentiment_tokenized_docs, sentiment_vocabulary)\n",
    "print(f\"Sentiment IDF computed for {len(sentiment_idf_dict)} n-grams\")\n",
    "\n",
    "# Compute TF-IDF for sentiment\n",
    "sentiment_tfidf_docs = [compute_tfidf(tf, sentiment_idf_dict) for tf in sentiment_tf_docs]\n",
    "print(\"Sentiment TF-IDF weights computed with n-grams\")\n",
    "\n",
    "# Create TF-IDF matrix for sentiment\n",
    "print(\"Creating sentiment TF-IDF matrix with n-grams...\")\n",
    "sentiment_tfidf_matrix = create_tfidf_matrix(sentiment_tfidf_docs, sentiment_vocabulary, sentiment_word2idx)\n",
    "print(f\"Sentiment TF-IDF Matrix: {sentiment_tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977746b",
   "metadata": {},
   "source": [
    "## 14. Prepare Sentiment Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca581f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sent = sentiment_tfidf_matrix\n",
    "y_sent = np.array(sentiment_labels)\n",
    "\n",
    "X_sent_train, X_sent_test, y_sent_train, y_sent_test = train_test_split(\n",
    "    X_sent, y_sent, test_size=0.2, random_state=42, stratify=y_sent\n",
    ")\n",
    "\n",
    "print(f\"Sentiment Training set: {X_sent_train.shape[0]} samples\")\n",
    "print(f\"Sentiment Test set: {X_sent_test.shape[0]} samples\")\n",
    "print(f\"N-gram features: {X_sent_train.shape[1]}\")\n",
    "print(f\"\\nSentiment distribution (train):\")\n",
    "print(pd.Series(y_sent_train).value_counts())\n",
    "print(f\"\\nSentiment distribution (test):\")\n",
    "print(pd.Series(y_sent_test).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abce1edd",
   "metadata": {},
   "source": [
    "## 15. Train Sentiment Classification Models with N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b382af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes for Sentiment\n",
    "sent_nb_model = MultinomialNB()\n",
    "sent_nb_model.fit(X_sent_train, y_sent_train)\n",
    "sent_nb_train_pred = sent_nb_model.predict(X_sent_train)\n",
    "sent_nb_test_pred = sent_nb_model.predict(X_sent_test)\n",
    "print(\"Sentiment Naive Bayes completed with n-grams\")\n",
    "\n",
    "# Linear SVM for Sentiment\n",
    "sent_svm_model = LinearSVC(C=1.0, random_state=42, max_iter=1000)\n",
    "sent_svm_model.fit(X_sent_train, y_sent_train)\n",
    "sent_svm_train_pred = sent_svm_model.predict(X_sent_train)\n",
    "sent_svm_test_pred = sent_svm_model.predict(X_sent_test)\n",
    "print(\"Sentiment Linear SVM completed with n-grams\")\n",
    "\n",
    "# Logistic Regression for Sentiment\n",
    "sent_lr_model = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial')\n",
    "sent_lr_model.fit(X_sent_train, y_sent_train)\n",
    "sent_lr_train_pred = sent_lr_model.predict(X_sent_train)\n",
    "sent_lr_test_pred = sent_lr_model.predict(X_sent_test)\n",
    "print(\"Sentiment Logistic Regression completed with n-grams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac6d75",
   "metadata": {},
   "source": [
    "## 16. Evaluate Sentiment Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Naive Bayes\n",
    "sent_nb_train_metrics = evaluate_model(y_sent_train, sent_nb_train_pred, \"Sentiment Naive Bayes (N-gram)\", \"Training\")\n",
    "sent_nb_test_metrics = evaluate_model(y_sent_test, sent_nb_test_pred, \"Sentiment Naive Bayes (N-gram)\", \"Test\")\n",
    "\n",
    "# Evaluate Linear SVM\n",
    "sent_svm_train_metrics = evaluate_model(y_sent_train, sent_svm_train_pred, \"Sentiment Linear SVM (N-gram)\", \"Training\")\n",
    "sent_svm_test_metrics = evaluate_model(y_sent_test, sent_svm_test_pred, \"Sentiment Linear SVM (N-gram)\", \"Test\")\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "sent_lr_train_metrics = evaluate_model(y_sent_train, sent_lr_train_pred, \"Sentiment Logistic Regression (N-gram)\", \"Training\")\n",
    "sent_lr_test_metrics = evaluate_model(y_sent_test, sent_lr_test_pred, \"Sentiment Logistic Regression (N-gram)\", \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2effc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for sentiment models\n",
    "sent_classes = sorted(list(set(y_sent_test)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Naive Bayes confusion matrix\n",
    "sent_nb_cm = confusion_matrix(y_sent_test, sent_nb_test_pred)\n",
    "sns.heatmap(sent_nb_cm, annot=True, fmt='d', cmap='Greens', ax=axes[0],\n",
    "            xticklabels=sent_classes, yticklabels=sent_classes)\n",
    "axes[0].set_title('Sentiment: Naive Bayes (N-gram)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "\n",
    "# SVM confusion matrix\n",
    "sent_svm_cm = confusion_matrix(y_sent_test, sent_svm_test_pred)\n",
    "sns.heatmap(sent_svm_cm, annot=True, fmt='d', cmap='Oranges', ax=axes[1],\n",
    "            xticklabels=sent_classes, yticklabels=sent_classes)\n",
    "axes[1].set_title('Sentiment: Linear SVM (N-gram)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "\n",
    "# Logistic Regression confusion matrix\n",
    "sent_lr_cm = confusion_matrix(y_sent_test, sent_lr_test_pred)\n",
    "sns.heatmap(sent_lr_cm, annot=True, fmt='d', cmap='Blues', ax=axes[2],\n",
    "            xticklabels=sent_classes, yticklabels=sent_classes)\n",
    "axes[2].set_title('Sentiment: Logistic Regression (N-gram)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Predicted')\n",
    "axes[2].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c9576",
   "metadata": {},
   "source": [
    "## 17. Sentiment Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e830b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_comparison_df = pd.DataFrame({\n",
    "    'Model': ['Naive Bayes', 'Linear SVM', 'Logistic Regression'],\n",
    "    'Train Accuracy': [sent_nb_train_metrics['accuracy'], sent_svm_train_metrics['accuracy'], sent_lr_train_metrics['accuracy']],\n",
    "    'Test Accuracy': [sent_nb_test_metrics['accuracy'], sent_svm_test_metrics['accuracy'], sent_lr_test_metrics['accuracy']],\n",
    "    'Test Precision': [sent_nb_test_metrics['precision'], sent_svm_test_metrics['precision'], sent_lr_test_metrics['precision']],\n",
    "    'Test Recall': [sent_nb_test_metrics['recall'], sent_svm_test_metrics['recall'], sent_lr_test_metrics['recall']],\n",
    "    'Test F1-Score': [sent_nb_test_metrics['f1'], sent_svm_test_metrics['f1'], sent_lr_test_metrics['f1']],\n",
    "    'N-gram Range': [str(NGRAM_RANGE)] * 3,\n",
    "    'Feature Count': [len(sentiment_vocabulary)] * 3\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SENTIMENT MODEL COMPARISON (WITH N-GRAMS)\")\n",
    "print(\"=\"*80)\n",
    "print(sent_comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb07000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment model comparison\n",
    "metrics = ['Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1-Score']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width, \n",
    "                [sent_nb_test_metrics['accuracy'], sent_nb_test_metrics['precision'], \n",
    "                 sent_nb_test_metrics['recall'], sent_nb_test_metrics['f1']], \n",
    "                width, label='Naive Bayes', color='lightgreen')\n",
    "rects2 = ax.bar(x, \n",
    "                [sent_svm_test_metrics['accuracy'], sent_svm_test_metrics['precision'], \n",
    "                 sent_svm_test_metrics['recall'], sent_svm_test_metrics['f1']], \n",
    "                width, label='Linear SVM', color='orange')\n",
    "rects3 = ax.bar(x + width, \n",
    "                [sent_lr_test_metrics['accuracy'], sent_lr_test_metrics['precision'], \n",
    "                 sent_lr_test_metrics['recall'], sent_lr_test_metrics['f1']], \n",
    "                width, label='Logistic Regression', color='skyblue')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title(f'Sentiment Model Performance Comparison (N-gram={NGRAM_RANGE})', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac47c9c",
   "metadata": {},
   "source": [
    "## 18. Save Sentiment Models and Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce4587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best sentiment model\n",
    "sent_best_model_name = 'Naive Bayes'\n",
    "sent_best_f1 = sent_nb_test_metrics['f1']\n",
    "sent_best_model = sent_nb_model\n",
    "\n",
    "if sent_svm_test_metrics['f1'] > sent_best_f1:\n",
    "    sent_best_model_name = 'Linear SVM'\n",
    "    sent_best_f1 = sent_svm_test_metrics['f1']\n",
    "    sent_best_model = sent_svm_model\n",
    "    \n",
    "if sent_lr_test_metrics['f1'] > sent_best_f1:\n",
    "    sent_best_model_name = 'Logistic Regression'\n",
    "    sent_best_f1 = sent_lr_test_metrics['f1']\n",
    "    sent_best_model = sent_lr_model\n",
    "\n",
    "print(f\"\\nBest Sentiment Model: {sent_best_model_name} (F1: {sent_best_f1:.4f})\")\n",
    "\n",
    "# Save sentiment models with n-gram suffix\n",
    "with open('models/sentiment_naive_bayes_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(sent_nb_model, f)\n",
    "    \n",
    "with open('models/sentiment_svm_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(sent_svm_model, f)\n",
    "    \n",
    "with open('models/sentiment_logistic_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(sent_lr_model, f)\n",
    "\n",
    "# Save best sentiment model\n",
    "with open('models/sentiment_model_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(sent_best_model, f)\n",
    "\n",
    "# Save sentiment vectorizer with n-gram range\n",
    "sentiment_vectorizer_data = {\n",
    "    'vocabulary': sentiment_vocabulary,\n",
    "    'word2idx': sentiment_word2idx,\n",
    "    'idf_dict': sentiment_idf_dict,\n",
    "    'ngram_range': NGRAM_RANGE\n",
    "}\n",
    "with open('models/sentiment_vectorizer_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(sentiment_vectorizer_data, f)\n",
    "\n",
    "# Save sentiment comparison\n",
    "sent_comparison_df.to_csv('output/sentiment_model_comparison_ngram.csv', index=False)\n",
    "\n",
    "# Save sentiment evaluation reports\n",
    "sent_reports = {\n",
    "    'naive_bayes': {\n",
    "        'train_metrics': sent_nb_train_metrics,\n",
    "        'test_metrics': sent_nb_test_metrics,\n",
    "        'classification_report': classification_report(y_sent_test, sent_nb_test_pred, output_dict=True, zero_division=0)\n",
    "    },\n",
    "    'svm': {\n",
    "        'train_metrics': sent_svm_train_metrics,\n",
    "        'test_metrics': sent_svm_test_metrics,\n",
    "        'classification_report': classification_report(y_sent_test, sent_svm_test_pred, output_dict=True, zero_division=0)\n",
    "    },\n",
    "    'logistic': {\n",
    "        'train_metrics': sent_lr_train_metrics,\n",
    "        'test_metrics': sent_lr_test_metrics,\n",
    "        'classification_report': classification_report(y_sent_test, sent_lr_test_pred, output_dict=True, zero_division=0)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('reports/sentiment_naive_bayes_ngram_report.json', 'w') as f:\n",
    "    json.dump(sent_reports['naive_bayes'], f, indent=2)\n",
    "    \n",
    "with open('reports/sentiment_svm_ngram_report.json', 'w') as f:\n",
    "    json.dump(sent_reports['svm'], f, indent=2)\n",
    "    \n",
    "with open('reports/sentiment_logistic_ngram_report.json', 'w') as f:\n",
    "    json.dump(sent_reports['logistic'], f, indent=2)\n",
    "\n",
    "print(\"✓ All sentiment models saved to models/ directory (with _ngram suffix)\")\n",
    "print(\"✓ Sentiment vectorizer saved to models/sentiment_vectorizer_ngram.pkl\")\n",
    "print(\"✓ Sentiment reports saved to reports/ directory\")\n",
    "print(f\"✓ Best sentiment model ({sent_best_model_name}) saved as models/sentiment_model_ngram.pkl\")\n",
    "print(f\"✓ N-gram range: {NGRAM_RANGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b3905",
   "metadata": {},
   "source": [
    "## 19. Final Summary: N-gram TF-IDF Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b48ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE N-GRAM TF-IDF PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📊 CATEGORY CLASSIFICATION (with N-grams):\")\n",
    "print(f\"  N-gram Range: {NGRAM_RANGE}\")\n",
    "print(f\"  Dataset: {len(documents)} documents\")\n",
    "print(f\"  Feature Count: {len(vocabulary):,} n-grams\")\n",
    "print(f\"  Train/Test Split: {X_train.shape[0]}/{X_test.shape[0]}\")\n",
    "print(f\"  Best Model: {best_model_name}\")\n",
    "print(f\"  Best F1-Score: {best_f1:.4f}\")\n",
    "\n",
    "print(\"\\n💭 SENTIMENT CLASSIFICATION (with N-grams):\")\n",
    "print(f\"  N-gram Range: {NGRAM_RANGE}\")\n",
    "print(f\"  Dataset: {len(sentiment_documents)} documents\")\n",
    "print(f\"  Feature Count: {len(sentiment_vocabulary):,} n-grams\")\n",
    "print(f\"  Train/Test Split: {X_sent_train.shape[0]}/{X_sent_test.shape[0]}\")\n",
    "print(f\"  Best Model: {sent_best_model_name}\")\n",
    "print(f\"  Best F1-Score: {sent_best_f1:.4f}\")\n",
    "\n",
    "print(\"\\n🔑 KEY ADVANTAGES OF N-GRAMS:\")\n",
    "print(f\"  ✓ Captures word sequences and context\")\n",
    "print(f\"  ✓ More discriminative features than unigrams alone\")\n",
    "print(f\"  ✓ Better understanding of phrases (e.g., 'இலங்கை அரசு')\")\n",
    "print(f\"  ✓ Improved classification performance\")\n",
    "print(f\"  ✓ Adjustable ngram_range parameter for flexibility\")\n",
    "\n",
    "print(\"\\n💾 SAVED ARTIFACTS:\")\n",
    "print(\"  Category Models:\")\n",
    "print(\"    - models/category_naive_bayes_ngram.pkl\")\n",
    "print(\"    - models/category_svm_ngram.pkl\")\n",
    "print(\"    - models/category_logistic_ngram.pkl\")\n",
    "print(\"    - models/category_model_ngram.pkl (best)\")\n",
    "print(\"    - models/category_vectorizer_ngram.pkl\")\n",
    "print(\"\\n  Sentiment Models:\")\n",
    "print(\"    - models/sentiment_naive_bayes_ngram.pkl\")\n",
    "print(\"    - models/sentiment_svm_ngram.pkl\")\n",
    "print(\"    - models/sentiment_logistic_ngram.pkl\")\n",
    "print(\"    - models/sentiment_model_ngram.pkl (best)\")\n",
    "print(\"    - models/sentiment_vectorizer_ngram.pkl\")\n",
    "print(\"\\n  Reports:\")\n",
    "print(\"    - output/model_comparison_ngram.csv\")\n",
    "print(\"    - output/sentiment_model_comparison_ngram.csv\")\n",
    "print(\"    - reports/*_ngram_report.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ N-GRAM TF-IDF PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
