{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea453cbd",
   "metadata": {},
   "source": [
    "# TF-IDF with N-gram Tokenization and Model Training\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Manual implementation of TF-IDF with **N-gram tokenization** (from scratch)\n",
    "2. Adjustable n-gram range parameter (default: unigrams + bigrams)\n",
    "3. Comparison with unigram-only approach\n",
    "4. Training machine learning models for text classification\n",
    "5. Model evaluation with comprehensive metrics\n",
    "\n",
    "**Key Enhancement:** N-gram tokenization captures word sequences (e.g., \"இலங்கை அரசு\") as features\n",
    "\n",
    "**Dataset:** Tamil news articles with categories and processed text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be12ec",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535cd04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"N-gram tokenization enabled ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0bdb86",
   "metadata": {},
   "source": [
    "## 2. Load the Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68228f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data from previous notebook\n",
    "df = pd.read_csv('output/processed_data.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3603d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data distribution\n",
    "print(\"Category distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "print(f\"\\nTotal samples: {len(df)}\")\n",
    "print(f\"Missing values:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8b3cf4",
   "metadata": {},
   "source": [
    "## 3. Prepare Text Data for TF-IDF\n",
    "\n",
    "We'll use the `cleaned_title` column which contains preprocessed Tamil text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a510e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the text column and target variable\n",
    "documents = df['cleaned_title'].fillna('').tolist()\n",
    "labels = df['category'].tolist()\n",
    "\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "print(f\"Total labels: {len(labels)}\")\n",
    "print(f\"\\nSample document: {documents[0]}\")\n",
    "print(f\"Sample label: {labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f64270c",
   "metadata": {},
   "source": [
    "## 4. TF-IDF Implementation from Scratch with N-grams\n",
    "\n",
    "### Step 1: N-gram Tokenization\n",
    "\n",
    "**Key Enhancement:** Instead of splitting only into single words (unigrams), we now generate word n-grams based on `ngram_range`.\n",
    "\n",
    "- **Unigrams (1-gram):** [\"இலங்கை\", \"அரசு\", \"தீர்மானம்\"]\n",
    "- **Bigrams (2-gram):** [\"இலங்கை அரசு\", \"அரசு தீர்மானம்\"]\n",
    "- **ngram_range=(1,2):** Both unigrams and bigrams combined\n",
    "\n",
    "This captures context and word sequences as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe2cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(tokens, ngram_range=(1, 2)):\n",
    "    \"\"\"\n",
    "    Generate n-grams from a list of tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of words (tokens)\n",
    "        ngram_range: Tuple (min_n, max_n) specifying the range of n-grams\n",
    "    \n",
    "    Returns:\n",
    "        List of n-gram strings\n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    min_n, max_n = ngram_range\n",
    "    \n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = ' '.join(tokens[i:i+n])\n",
    "            ngrams.append(ngram)\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "def tokenize_with_ngrams(text, ngram_range=(1, 2)):\n",
    "    \"\"\"\n",
    "    Tokenize text and generate n-grams.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    return generate_ngrams(tokens, ngram_range)\n",
    "\n",
    "# Example of n-gram tokenization\n",
    "sample_text = documents[0] if documents[0] else \"இலங்கை அரசு தீர்மானம்\"\n",
    "sample_tokens = sample_text.split()\n",
    "\n",
    "print(\"Original tokens (unigrams only):\")\n",
    "print(sample_tokens[:10])\n",
    "\n",
    "print(\"\\nN-grams with ngram_range=(1, 2):\")\n",
    "ngrams_sample = generate_ngrams(sample_tokens, ngram_range=(1, 2))\n",
    "print(ngrams_sample[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurable n-gram range parameter\n",
    "NGRAM_RANGE = (1, 2)  # Default: unigrams + bigrams\n",
    "\n",
    "# Apply n-gram tokenization to all documents\n",
    "tokenized_docs = [tokenize_with_ngrams(doc, ngram_range=NGRAM_RANGE) for doc in documents]\n",
    "print(f\"Tokenized {len(tokenized_docs)} documents with ngram_range={NGRAM_RANGE}\")\n",
    "print(f\"Sample tokenized document length: {len(tokenized_docs[0])} n-grams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8558e",
   "metadata": {},
   "source": [
    "### Step 2: Build Vocabulary with N-grams\n",
    "\n",
    "Create a vocabulary including all unique n-grams in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3566aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "for doc in tokenized_docs:\n",
    "    vocabulary.update(doc)\n",
    "\n",
    "vocabulary = sorted(list(vocabulary))\n",
    "word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "print(f\"Vocabulary size with ngram_range={NGRAM_RANGE}: {len(vocabulary)} unique n-grams\")\n",
    "print(f\"\\nSample vocabulary entries:\")\n",
    "print(vocabulary[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2a641a",
   "metadata": {},
   "source": [
    "### Step 3: Compute Term Frequency (TF) for N-grams\n",
    "\n",
    "**Term Frequency (TF)** measures how frequently an n-gram appears in a document.\n",
    "\n",
    "Formula: `TF(t, d) = (Number of times n-gram t appears in document d) / (Total number of n-grams in document d)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa306544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(tokenized_doc):\n",
    "    \"\"\"Compute term frequency for n-grams: TF(t) = count(t) / total_ngrams\"\"\"\n",
    "    tf_dict = {}\n",
    "    doc_length = len(tokenized_doc)\n",
    "    if doc_length == 0:\n",
    "        return tf_dict\n",
    "    term_counts = Counter(tokenized_doc)\n",
    "    for term, count in term_counts.items():\n",
    "        tf_dict[term] = count / doc_length\n",
    "    return tf_dict\n",
    "\n",
    "tf_docs = [compute_tf(doc) for doc in tokenized_docs]\n",
    "print(\"Term Frequency computed for n-grams\")\n",
    "print(f\"Sample TF dict entries: {list(tf_docs[0].items())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a81603e",
   "metadata": {},
   "source": [
    "### Step 4: Compute Inverse Document Frequency (IDF) for N-grams\n",
    "\n",
    "**Inverse Document Frequency (IDF)** measures how important an n-gram is across the entire corpus.\n",
    "\n",
    "Formula: `IDF(t) = log(Total number of documents / Number of documents containing n-gram t)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14b2256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(tokenized_docs, vocabulary):\n",
    "    \"\"\"Compute IDF for n-grams: IDF(t) = log(N / df(t))\"\"\"\n",
    "    N = len(tokenized_docs)\n",
    "    idf_dict = {}\n",
    "    for word in vocabulary:\n",
    "        doc_count = sum(1 for doc in tokenized_docs if word in doc)\n",
    "        idf_dict[word] = math.log(N / (doc_count + 1))\n",
    "    return idf_dict\n",
    "\n",
    "print(\"Computing IDF for n-grams...\")\n",
    "idf_dict = compute_idf(tokenized_docs, vocabulary)\n",
    "print(f\"IDF computed for {len(idf_dict)} n-grams\")\n",
    "print(f\"Sample IDF values: {list(idf_dict.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23794280",
   "metadata": {},
   "source": [
    "### Step 5: Compute TF-IDF Weights for N-grams\n",
    "\n",
    "**TF-IDF** combines both TF and IDF to get the final weight for each n-gram in each document.\n",
    "\n",
    "Formula: `TF-IDF(t, d) = TF(t, d) × IDF(t)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27df7c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(tf_dict, idf_dict):\n",
    "    \"\"\"Compute TF-IDF for n-grams: TF-IDF(t, d) = TF(t, d) × IDF(t)\"\"\"\n",
    "    tfidf_dict = {}\n",
    "    for term, tf_value in tf_dict.items():\n",
    "        tfidf_dict[term] = tf_value * idf_dict.get(term, 0)\n",
    "    return tfidf_dict\n",
    "\n",
    "tfidf_docs = [compute_tfidf(tf, idf_dict) for tf in tf_docs]\n",
    "print(\"TF-IDF weights computed for n-grams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa8f495",
   "metadata": {},
   "source": [
    "### Step 6: Create TF-IDF Matrix with N-grams\n",
    "\n",
    "Convert the TF-IDF dictionaries into a matrix representation where:\n",
    "- Rows represent documents\n",
    "- Columns represent n-grams in vocabulary\n",
    "- Values are TF-IDF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_matrix(tfidf_docs, vocabulary, word2idx):\n",
    "    \"\"\"Create TF-IDF matrix: (n_documents, n_vocabulary)\"\"\"\n",
    "    n_docs = len(tfidf_docs)\n",
    "    n_vocab = len(vocabulary)\n",
    "    tfidf_matrix = np.zeros((n_docs, n_vocab))\n",
    "    \n",
    "    for doc_idx, tfidf_dict in enumerate(tfidf_docs):\n",
    "        for term, tfidf_value in tfidf_dict.items():\n",
    "            if term in word2idx:\n",
    "                term_idx = word2idx[term]\n",
    "                tfidf_matrix[doc_idx, term_idx] = tfidf_value\n",
    "    \n",
    "    return tfidf_matrix\n",
    "\n",
    "print(\"Creating TF-IDF matrix with n-grams...\")\n",
    "tfidf_matrix_custom = create_tfidf_matrix(tfidf_docs, vocabulary, word2idx)\n",
    "print(f\"TF-IDF Matrix: {tfidf_matrix_custom.shape}\")\n",
    "print(f\"  - Documents: {tfidf_matrix_custom.shape[0]}\")\n",
    "print(f\"  - N-gram features: {tfidf_matrix_custom.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a57c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(tfidf_matrix_custom[:10, :20], aspect='auto', cmap='YlOrRd')\n",
    "plt.colorbar(label='TF-IDF Weight')\n",
    "plt.xlabel('N-gram Feature Index')\n",
    "plt.ylabel('Document Index')\n",
    "plt.title(f'TF-IDF Matrix Visualization (with N-grams, range={NGRAM_RANGE})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9558b4",
   "metadata": {},
   "source": [
    "## 5. TF-IDF Matrix Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e964bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display TF-IDF matrix statistics\n",
    "print(f\"TF-IDF Matrix Statistics (with N-grams {NGRAM_RANGE}):\")\n",
    "print(f\"  Shape: {tfidf_matrix_custom.shape}\")\n",
    "print(f\"  Mean: {tfidf_matrix_custom.mean():.6f}\")\n",
    "print(f\"  Max: {tfidf_matrix_custom.max():.6f}\")\n",
    "print(f\"  Sparsity: {(tfidf_matrix_custom == 0).sum() / tfidf_matrix_custom.size * 100:.2f}%\")\n",
    "print(f\"\\nNote: N-gram features = {tfidf_matrix_custom.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1e0e2",
   "metadata": {},
   "source": [
    "## 6. Compare Unigrams vs N-grams (Feature Count)\n",
    "\n",
    "Before training models, let's compare the vocabulary size difference between unigrams and n-grams."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
