{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea453cbd",
   "metadata": {},
   "source": [
    "# TF-IDF with N-gram Tokenization and Model Training\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Optimized Manual TF-IDF Implementation** with N-gram tokenization (from scratch)\n",
    "2. **Sparse matrix support** for memory efficiency (scipy.sparse)\n",
    "3. Adjustable n-gram range parameter (default: unigrams + bigrams)\n",
    "4. **Vocabulary filtering** (top 10K n-grams, min_df=3, max_df=80%)\n",
    "5. **Manual Naive Bayes** implementation from scratch\n",
    "6. Comparison with unigram-only approach\n",
    "7. Model evaluation with comprehensive metrics\n",
    "\n",
    "**Key Enhancements:**\n",
    "- тЬЕ N-gram tokenization captures word sequences (e.g., \"роЗро▓роЩрпНроХрпИ роЕро░роЪрпБ\")\n",
    "- тЬЕ Vocabulary filtering (top 10K n-grams, min_df=3, max_df=80%)\n",
    "- тЬЕ Sparse matrices (float32) for memory efficiency\n",
    "- тЬЕ Manual Naive Bayes implementation\n",
    "\n",
    "**Dataset:** Tamil news articles with categories and processed text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be12ec",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "535cd04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "N-gram tokenization enabled тЬУ\n",
      "Sparse matrix support enabled тЬУ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"N-gram tokenization enabled тЬУ\")\n",
    "print(\"Sparse matrix support enabled тЬУ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0bdb86",
   "metadata": {},
   "source": [
    "## 2. Load the Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68228f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (18447, 5)\n",
      "\n",
      "Columns: ['category', 'processed_title', 'cleaned_title', 'tokenized_title', 'title']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>processed_title</th>\n",
       "      <th>cleaned_title</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tamilnadu</td>\n",
       "      <td>роорпЗроХродро╛родрпБ ро╡ро┐ро╡роХро╛ро░роорпН родрооро┐ро┤роХ роХро░рпНроиро╛роЯроХро╛ роорпБродро▓роорпИроЪрпНроЪро░рпН роиро┐родро┐ройрпН роХроЯрпНроХро░ро┐ роХроЯро┐родроорпН</td>\n",
       "      <td>роорпЗроХродро╛родрпБ ро╡ро┐ро╡роХро╛ро░роорпН родрооро┐ро┤роХ роХро░рпНроиро╛роЯроХро╛ роорпБродро▓роорпИроЪрпНроЪро░рпНроХро│рпБроХрпНроХрпБ роиро┐родро┐ройрпН роХроЯрпНроХро░ро┐ роХроЯро┐родроорпН</td>\n",
       "      <td>роорпЗроХродро╛родрпБ ро╡ро┐ро╡роХро╛ро░роорпН родрооро┐ро┤роХ роХро░рпНроиро╛роЯроХро╛ роорпБродро▓роорпИроЪрпНроЪро░рпНроХро│рпБроХрпНроХрпБ роиро┐родро┐ройрпН роХроЯрпНроХро░ро┐ роХроЯро┐родроорпН</td>\n",
       "      <td>роорпЗроХродро╛родрпБ ро╡ро┐ро╡роХро╛ро░роорпН: родрооро┐ро┤роХ, роХро░рпНроиро╛роЯроХро╛ роорпБродро▓роорпИроЪрпНроЪро░рпНроХро│рпБроХрпНроХрпБ роиро┐родро┐ройрпН роХроЯрпНроХро░ро┐ роХроЯро┐родроорпН</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>рокроирпНродрпБро╡рпАроЪрпНроЪро╛ро│ро░рпН роРрокро┐роОро▓рпН ро╡ро┐ро│рпИропро╛роЯро▓ро╛рооро╛ роОродро┐ро░рпЖродро┐ро░рпН роХро░рпБродрпНродро┐ро▓рпН родрпЛройро┐ роХрпБроорпНрокро│рпЗ</td>\n",
       "      <td>рокроирпНродрпБро╡рпАроЪрпНроЪро╛ро│ро░рпНроХро│рпН роРрокро┐роОро▓рпН ро╡ро┐ро│рпИропро╛роЯро▓ро╛рооро╛ роОродро┐ро░рпЖродро┐ро░рпН роХро░рпБродрпНродро┐ро▓рпН родрпЛройро┐ роХрпБроорпНрокро│рпЗ</td>\n",
       "      <td>рокроирпНродрпБро╡рпАроЪрпНроЪро╛ро│ро░рпНроХро│рпН роРрокро┐роОро▓рпН ро╡ро┐ро│рпИропро╛роЯро▓ро╛рооро╛ роОродро┐ро░рпЖродро┐ро░рпН роХро░рпБродрпНродро┐ро▓рпН родрпЛройро┐ роХрпБроорпНрокро│рпЗ</td>\n",
       "      <td>рокроирпНродрпБро╡рпАроЪрпНроЪро╛ро│ро░рпНроХро│рпН роРрокро┐роОро▓рпН ро╡ро┐ро│рпИропро╛роЯро▓ро╛рооро╛? - роОродро┐ро░рпЖродро┐ро░рпН роХро░рпБродрпНродро┐ро▓рпН родрпЛройро┐-роХрпБроорпНрокро│рпЗ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tamilnadu</td>\n",
       "      <td>роХройрооро┤рпИ роОроЪрпНроЪро░ро┐роХрпНроХрпИ роиро╛ро│рпИ рокро│рпНро│ро┐ роХро▓рпНро▓рпВро░ро┐ ро╡ро┐роЯрпБроорпБро▒рпИ роОроЩрпНроХрпЖро▓рпНро▓ро╛роорпН родрпЖро░ро┐ропрпБрооро╛</td>\n",
       "      <td>роХройрооро┤рпИ роОроЪрпНроЪро░ро┐роХрпНроХрпИ роиро╛ро│рпИ рокро│рпНро│ро┐ роХро▓рпНро▓рпВро░ро┐роХро│рпБроХрпНроХрпБ ро╡ро┐роЯрпБроорпБро▒рпИ роОроЩрпНроХрпЖро▓рпНро▓ро╛роорпН родрпЖро░ро┐ропрпБрооро╛</td>\n",
       "      <td>роХройрооро┤рпИ роОроЪрпНроЪро░ро┐роХрпНроХрпИ роиро╛ро│рпИ рокро│рпНро│ро┐ роХро▓рпНро▓рпВро░ро┐роХро│рпБроХрпНроХрпБ ро╡ро┐роЯрпБроорпБро▒рпИ роОроЩрпНроХрпЖро▓рпНро▓ро╛роорпН родрпЖро░ро┐ропрпБрооро╛</td>\n",
       "      <td>роХройрооро┤рпИ роОроЪрпНроЪро░ро┐роХрпНроХрпИ | роиро╛ро│рпИ рокро│рпНро│ро┐, роХро▓рпНро▓рпВро░ро┐роХро│рпБроХрпНроХрпБ ро╡ро┐роЯрпБроорпБро▒рпИ.. роОроЩрпНроХрпЖро▓рпНро▓ро╛роорпН родрпЖро░ро┐ропрпБрооро╛?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tamilnadu</td>\n",
       "      <td>родро╡рпЖроХро╡рпИ роЖрогрпНроЯро╡ройро╛ро▓рпБроорпН роХро╛рокрпНрокро╛ро▒рпНро▒ роорпБроЯро┐ропро╛родрпБ ро╡ро┐роЬропрпН роЖро░рпНрокро┐ роЙродропроХрпБрооро╛ро░рпН роЕроЯрпНро╡рпИро╕рпН</td>\n",
       "      <td>родро╡рпЖроХро╡рпИ роЖрогрпНроЯро╡ройро╛ро▓рпБроорпН роХро╛рокрпНрокро╛ро▒рпНро▒ роорпБроЯро┐ропро╛родрпБ ро╡ро┐роЬропрпНроХрпНроХрпБ роЖро░рпНрокро┐ роЙродропроХрпБрооро╛ро░рпН роЕроЯрпНро╡рпИро╕рпН</td>\n",
       "      <td>родро╡рпЖроХро╡рпИ роЖрогрпНроЯро╡ройро╛ро▓рпБроорпН роХро╛рокрпНрокро╛ро▒рпНро▒ роорпБроЯро┐ропро╛родрпБ ро╡ро┐роЬропрпНроХрпНроХрпБ роЖро░рпНрокро┐ роЙродропроХрпБрооро╛ро░рпН роЕроЯрпНро╡рпИро╕рпН</td>\n",
       "      <td>родро╡рпЖроХро╡рпИ роЖрогрпНроЯро╡ройро╛ро▓рпБроорпН роХро╛рокрпНрокро╛ро▒рпНро▒ роорпБроЯро┐ропро╛родрпБ - ро╡ро┐роЬропрпНроХрпНроХрпБ роЖро░рпНрокро┐ роЙродропроХрпБрооро╛ро░рпН роЕроЯрпНро╡рпИро╕рпН</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tamilnadu</td>\n",
       "      <td>роЖро┤рпН роХро╛ро▒рпНро▒ро┤рпБ родро╛ро┤рпНро╡рпБрокрпНрокроХрпБродро┐ роОродро┐ро░рпКро▓ро┐ рооро╛ро╡роЯрпНроЯроЩрпН роЕро▓ро░рпНроЯрпН</td>\n",
       "      <td>роЖро┤рпНроирпНрод роХро╛ро▒рпНро▒ро┤рпБродрпНрод родро╛ро┤рпНро╡рпБрокрпНрокроХрпБродро┐ роОродро┐ро░рпКро▓ро┐ рооро╛ро╡роЯрпНроЯроЩрпНроХро│рпБроХрпНроХрпБ роЕро▓ро░рпНроЯрпН</td>\n",
       "      <td>роЖро┤рпНроирпНрод роХро╛ро▒рпНро▒ро┤рпБродрпНрод родро╛ро┤рпНро╡рпБрокрпНрокроХрпБродро┐ роОродро┐ро░рпКро▓ро┐ рооро╛ро╡роЯрпНроЯроЩрпНроХро│рпБроХрпНроХрпБ роЕро▓ро░рпНроЯрпН</td>\n",
       "      <td>роЖро┤рпНроирпНрод роХро╛ро▒рпНро▒ро┤рпБродрпНрод родро╛ро┤рпНро╡рпБрокрпНрокроХрпБродро┐ роОродро┐ро░рпКро▓ро┐.. 8 рооро╛ро╡роЯрпНроЯроЩрпНроХро│рпБроХрпНроХрпБ RED роЕро▓ро░рпНроЯрпН..</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category  \\\n",
       "0  tamilnadu   \n",
       "1     sports   \n",
       "2  tamilnadu   \n",
       "3  tamilnadu   \n",
       "4  tamilnadu   \n",
       "\n",
       "                                                       processed_title  \\\n",
       "0     роорпЗроХродро╛родрпБ ро╡ро┐ро╡роХро╛ро░роорпН родрооро┐ро┤роХ роХро░рпНроиро╛роЯроХро╛ роорпБродро▓роорпИроЪрпНроЪро░рпН роиро┐родро┐ройрпН роХроЯрпНроХро░ро┐ роХроЯро┐родроорпН   \n",
       "1   рокроирпНродрпБро╡рпАроЪрпНроЪро╛ро│ро░рпН роРрокро┐роОро▓рпН ро╡ро┐ро│рпИропро╛роЯро▓ро╛рооро╛ роОродро┐ро░рпЖродро┐ро░рпН роХро░рпБродрпНродро┐ро▓рпН родрпЛройро┐ роХрпБроорпНрокро│рпЗ   \n",
       "2    роХройрооро┤рпИ роОроЪрпНроЪро░ро┐роХрпНроХрпИ роиро╛ро│рпИ рокро│рпНро│ро┐ роХро▓рпНро▓рпВро░ро┐ ро╡ро┐роЯрпБроорпБро▒рпИ роОроЩрпНроХрпЖро▓рпНро▓ро╛роорпН родрпЖро░ро┐ропрпБрооро╛   \n",
       "3  родро╡рпЖроХро╡рпИ роЖрогрпНроЯро╡ройро╛ро▓рпБроорпН роХро╛рокрпНрокро╛ро▒рпНро▒ роорпБроЯро┐ропро╛родрпБ ро╡ро┐роЬропрпН роЖро░рпНрокро┐ роЙродропроХрпБрооро╛ро░рпН роЕроЯрпНро╡рпИро╕рпН   \n",
       "4                    роЖро┤рпН роХро╛ро▒рпНро▒ро┤рпБ родро╛ро┤рпНро╡рпБрокрпНрокроХрпБродро┐ роОродро┐ро░рпКро▓ро┐ рооро╛ро╡роЯрпНроЯроЩрпН роЕро▓ро░рпНроЯрпН   \n",
       "\n",
       "                                                              cleaned_title  \\\n",
       "0   роорпЗроХродро╛родрпБ ро╡ро┐ро╡роХро╛ро░роорпН родрооро┐ро┤роХ роХро░рпНроиро╛роЯроХро╛ роорпБродро▓роорпИроЪрпНроЪро░рпНроХро│рпБроХрпНроХрпБ роиро┐родро┐ройрпН роХроЯрпНроХро░ро┐ роХроЯро┐родроорпН   \n",
       "1     рокроирпНродрпБро╡рпАроЪрпНроЪро╛ро│ро░рпНроХро│рпН роРрокро┐роОро▓рпН ро╡ро┐ро│рпИропро╛роЯро▓ро╛рооро╛ роОродро┐ро░рпЖродро┐ро░рпН роХро░рпБродрпНродро┐ро▓рпН родрпЛройро┐ роХрпБроорпНрокро│рпЗ   \n",
       "2  роХройрооро┤рпИ роОроЪрпНроЪро░ро┐роХрпНроХрпИ роиро╛ро│рпИ рокро│рпНро│ро┐ роХро▓рпНро▓рпВро░ро┐роХро│рпБроХрпНроХрпБ ро╡ро┐роЯрпБроорпБро▒рпИ роОроЩрпНроХрпЖро▓рпНро▓ро╛роорпН родрпЖро░ро┐ропрпБрооро╛   \n",
       "3   родро╡рпЖроХро╡рпИ роЖрогрпНроЯро╡ройро╛ро▓рпБроорпН роХро╛рокрпНрокро╛ро▒рпНро▒ роорпБроЯро┐ропро╛родрпБ ро╡ро┐роЬропрпНроХрпНроХрпБ роЖро░рпНрокро┐ роЙродропроХрпБрооро╛ро░рпН роЕроЯрпНро╡рпИро╕рпН   \n",
       "4            роЖро┤рпНроирпНрод роХро╛ро▒рпНро▒ро┤рпБродрпНрод родро╛ро┤рпНро╡рпБрокрпНрокроХрпБродро┐ роОродро┐ро░рпКро▓ро┐ рооро╛ро╡роЯрпНроЯроЩрпНроХро│рпБроХрпНроХрпБ роЕро▓ро░рпНроЯрпН   \n",
       "\n",
       "                                                            tokenized_title  \\\n",
       "0   роорпЗроХродро╛родрпБ ро╡ро┐ро╡роХро╛ро░роорпН родрооро┐ро┤роХ роХро░рпНроиро╛роЯроХро╛ роорпБродро▓роорпИроЪрпНроЪро░рпНроХро│рпБроХрпНроХрпБ роиро┐родро┐ройрпН роХроЯрпНроХро░ро┐ роХроЯро┐родроорпН   \n",
       "1     рокроирпНродрпБро╡рпАроЪрпНроЪро╛ро│ро░рпНроХро│рпН роРрокро┐роОро▓рпН ро╡ро┐ро│рпИропро╛роЯро▓ро╛рооро╛ роОродро┐ро░рпЖродро┐ро░рпН роХро░рпБродрпНродро┐ро▓рпН родрпЛройро┐ роХрпБроорпНрокро│рпЗ   \n",
       "2  роХройрооро┤рпИ роОроЪрпНроЪро░ро┐роХрпНроХрпИ роиро╛ро│рпИ рокро│рпНро│ро┐ роХро▓рпНро▓рпВро░ро┐роХро│рпБроХрпНроХрпБ ро╡ро┐роЯрпБроорпБро▒рпИ роОроЩрпНроХрпЖро▓рпНро▓ро╛роорпН родрпЖро░ро┐ропрпБрооро╛   \n",
       "3   родро╡рпЖроХро╡рпИ роЖрогрпНроЯро╡ройро╛ро▓рпБроорпН роХро╛рокрпНрокро╛ро▒рпНро▒ роорпБроЯро┐ропро╛родрпБ ро╡ро┐роЬропрпНроХрпНроХрпБ роЖро░рпНрокро┐ роЙродропроХрпБрооро╛ро░рпН роЕроЯрпНро╡рпИро╕рпН   \n",
       "4            роЖро┤рпНроирпНрод роХро╛ро▒рпНро▒ро┤рпБродрпНрод родро╛ро┤рпНро╡рпБрокрпНрокроХрпБродро┐ роОродро┐ро░рпКро▓ро┐ рооро╛ро╡роЯрпНроЯроЩрпНроХро│рпБроХрпНроХрпБ роЕро▓ро░рпНроЯрпН   \n",
       "\n",
       "                                                                            title  \n",
       "0       роорпЗроХродро╛родрпБ ро╡ро┐ро╡роХро╛ро░роорпН: родрооро┐ро┤роХ, роХро░рпНроиро╛роЯроХро╛ роорпБродро▓роорпИроЪрпНроЪро░рпНроХро│рпБроХрпНроХрпБ роиро┐родро┐ройрпН роХроЯрпНроХро░ро┐ роХроЯро┐родроорпН  \n",
       "1        рокроирпНродрпБро╡рпАроЪрпНроЪро╛ро│ро░рпНроХро│рпН роРрокро┐роОро▓рпН ро╡ро┐ро│рпИропро╛роЯро▓ро╛рооро╛? - роОродро┐ро░рпЖродро┐ро░рпН роХро░рпБродрпНродро┐ро▓рпН родрпЛройро┐-роХрпБроорпНрокро│рпЗ  \n",
       "2  роХройрооро┤рпИ роОроЪрпНроЪро░ро┐роХрпНроХрпИ | роиро╛ро│рпИ рокро│рпНро│ро┐, роХро▓рпНро▓рпВро░ро┐роХро│рпБроХрпНроХрпБ ро╡ро┐роЯрпБроорпБро▒рпИ.. роОроЩрпНроХрпЖро▓рпНро▓ро╛роорпН родрпЖро░ро┐ропрпБрооро╛?  \n",
       "3       родро╡рпЖроХро╡рпИ роЖрогрпНроЯро╡ройро╛ро▓рпБроорпН роХро╛рокрпНрокро╛ро▒рпНро▒ роорпБроЯро┐ропро╛родрпБ - ро╡ро┐роЬропрпНроХрпНроХрпБ роЖро░рпНрокро┐ роЙродропроХрпБрооро╛ро░рпН роЕроЯрпНро╡рпИро╕рпН  \n",
       "4        роЖро┤рпНроирпНрод роХро╛ро▒рпНро▒ро┤рпБродрпНрод родро╛ро┤рпНро╡рпБрокрпНрокроХрпБродро┐ роОродро┐ро░рпКро▓ро┐.. 8 рооро╛ро╡роЯрпНроЯроЩрпНроХро│рпБроХрпНроХрпБ RED роЕро▓ро░рпНроЯрпН..  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the processed data from previous notebook\n",
    "df = pd.read_csv('output/processed_data.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3603d0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category distribution:\n",
      "category\n",
      "india                                     2191\n",
      "world                                     1893\n",
      "cinema                                    1811\n",
      "sports                                    1748\n",
      "crime                                     1627\n",
      "tamilnadu                                 1589\n",
      "business                                  1304\n",
      "trending                                  1271\n",
      "technology                                1225\n",
      "features                                  1196\n",
      "health                                     841\n",
      "environment                                639\n",
      "agriculture                                613\n",
      "spiritual                                  245\n",
      "lifestyle                                  103\n",
      "motor                                       45\n",
      "coronavirus                                 38\n",
      "ampstories                                  30\n",
      "women                                       20\n",
      "employment-news-in-tamil-latest-update      18\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total samples: 18447\n",
      "Missing values:\n",
      "category           0\n",
      "processed_title    0\n",
      "cleaned_title      0\n",
      "tokenized_title    0\n",
      "title              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check data distribution\n",
    "print(\"Category distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "print(f\"\\nTotal samples: {len(df)}\")\n",
    "print(f\"Missing values:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8b3cf4",
   "metadata": {},
   "source": [
    "## 3. Prepare Text Data for TF-IDF\n",
    "\n",
    "We'll use the `cleaned_title` column which contains preprocessed Tamil text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a510e9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 18447\n",
      "Total labels: 18447\n",
      "\n",
      "Sample document: роорпЗроХродро╛родрпБ ро╡ро┐ро╡роХро╛ро░роорпН родрооро┐ро┤роХ роХро░рпНроиро╛роЯроХро╛ роорпБродро▓роорпИроЪрпНроЪро░рпНроХро│рпБроХрпНроХрпБ роиро┐родро┐ройрпН роХроЯрпНроХро░ро┐ роХроЯро┐родроорпН\n",
      "Sample label: tamilnadu\n"
     ]
    }
   ],
   "source": [
    "# Select the text column and target variable\n",
    "documents = df['cleaned_title'].fillna('').tolist()\n",
    "labels = df['category'].tolist()\n",
    "\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "print(f\"Total labels: {len(labels)}\")\n",
    "print(f\"\\nSample document: {documents[0]}\")\n",
    "print(f\"Sample label: {labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f64270c",
   "metadata": {},
   "source": [
    "## 4. TF-IDF Implementation from Scratch with N-grams\n",
    "\n",
    "### Step 1: N-gram Tokenization\n",
    "\n",
    "**Key Enhancement:** Instead of splitting only into single words (unigrams), we now generate word n-grams based on `ngram_range`.\n",
    "\n",
    "- **Unigrams (1-gram):** [\"роЗро▓роЩрпНроХрпИ\", \"роЕро░роЪрпБ\", \"родрпАро░рпНрооро╛ройроорпН\"]\n",
    "- **Bigrams (2-gram):** [\"роЗро▓роЩрпНроХрпИ роЕро░роЪрпБ\", \"роЕро░роЪрпБ родрпАро░рпНрооро╛ройроорпН\"]\n",
    "- **ngram_range=(1,2):** Both unigrams and bigrams combined\n",
    "\n",
    "This captures context and word sequences as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1efe2cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens (unigrams only):\n",
      "['роорпЗроХродро╛родрпБ', 'ро╡ро┐ро╡роХро╛ро░роорпН', 'родрооро┐ро┤роХ', 'роХро░рпНроиро╛роЯроХро╛', 'роорпБродро▓роорпИроЪрпНроЪро░рпНроХро│рпБроХрпНроХрпБ', 'роиро┐родро┐ройрпН', 'роХроЯрпНроХро░ро┐', 'роХроЯро┐родроорпН']\n",
      "\n",
      "N-grams with ngram_range=(1, 2):\n",
      "['роорпЗроХродро╛родрпБ', 'ро╡ро┐ро╡роХро╛ро░роорпН', 'родрооро┐ро┤роХ', 'роХро░рпНроиро╛роЯроХро╛', 'роорпБродро▓роорпИроЪрпНроЪро░рпНроХро│рпБроХрпНроХрпБ', 'роиро┐родро┐ройрпН', 'роХроЯрпНроХро░ро┐', 'роХроЯро┐родроорпН', 'роорпЗроХродро╛родрпБ ро╡ро┐ро╡роХро╛ро░роорпН', 'ро╡ро┐ро╡роХро╛ро░роорпН родрооро┐ро┤роХ', 'родрооро┐ро┤роХ роХро░рпНроиро╛роЯроХро╛', 'роХро░рпНроиро╛роЯроХро╛ роорпБродро▓роорпИроЪрпНроЪро░рпНроХро│рпБроХрпНроХрпБ', 'роорпБродро▓роорпИроЪрпНроЪро░рпНроХро│рпБроХрпНроХрпБ роиро┐родро┐ройрпН', 'роиро┐родро┐ройрпН роХроЯрпНроХро░ро┐', 'роХроЯрпНроХро░ро┐ роХроЯро┐родроорпН']\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(tokens, ngram_range=(1, 2)):\n",
    "    \"\"\"\n",
    "    Generate n-grams from a list of tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of words (tokens)\n",
    "        ngram_range: Tuple (min_n, max_n) specifying the range of n-grams\n",
    "    \n",
    "    Returns:\n",
    "        List of n-gram strings\n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    min_n, max_n = ngram_range\n",
    "    \n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = ' '.join(tokens[i:i+n])\n",
    "            ngrams.append(ngram)\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "def tokenize_with_ngrams(text, ngram_range=(1, 2)):\n",
    "    \"\"\"\n",
    "    Tokenize text and generate n-grams.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    return generate_ngrams(tokens, ngram_range)\n",
    "\n",
    "# Example of n-gram tokenization\n",
    "sample_text = documents[0] if documents[0] else \"роЗро▓роЩрпНроХрпИ роЕро░роЪрпБ родрпАро░рпНрооро╛ройроорпН\"\n",
    "sample_tokens = sample_text.split()\n",
    "\n",
    "print(\"Original tokens (unigrams only):\")\n",
    "print(sample_tokens[:10])\n",
    "\n",
    "print(\"\\nN-grams with ngram_range=(1, 2):\")\n",
    "ngrams_sample = generate_ngrams(sample_tokens, ngram_range=(1, 2))\n",
    "print(ngrams_sample[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "048f453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 18447 documents with ngram_range=(1, 2)\n",
      "Sample tokenized document length: 15 n-grams\n"
     ]
    }
   ],
   "source": [
    "# Configurable n-gram range parameter\n",
    "NGRAM_RANGE = (1, 2)  # Default: unigrams + bigrams\n",
    "\n",
    "# Apply n-gram tokenization to all documents\n",
    "tokenized_docs = [tokenize_with_ngrams(doc, ngram_range=NGRAM_RANGE) for doc in documents]\n",
    "print(f\"Tokenized {len(tokenized_docs)} documents with ngram_range={NGRAM_RANGE}\")\n",
    "print(f\"Sample tokenized document length: {len(tokenized_docs[0])} n-grams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8558e",
   "metadata": {},
   "source": [
    "### Step 2: Build Optimized Vocabulary with N-grams\n",
    "\n",
    "Create a vocabulary with filtering:\n",
    "1. Compute global n-gram frequencies across all documents\n",
    "2. Keep only top 10,000 most frequent n-grams\n",
    "3. Remove n-grams in < 3 documents (too rare)\n",
    "4. Remove n-grams in > 80% of documents (too common)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3566aac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimized n-gram vocabulary...\n",
      "Total unique n-grams before filtering: 144438\n",
      "N-grams after DF filtering: 13215\n",
      "\n",
      "============================================================\n",
      "Optimized N-gram Vocabulary Statistics:\n",
      "  N-gram range: (1, 2)\n",
      "  Final vocabulary size: 10,000 unique n-grams\n",
      "  Max features limit: 10,000\n",
      "  Min document frequency: 3\n",
      "  Max document frequency: 14757 (80%)\n",
      "  Reduction: 134,438 n-grams removed\n",
      "============================================================\n",
      "\n",
      "Sample vocabulary entries:\n",
      "['роГрокрогрпНроЯрпН', 'роГрокро░рпНро╕рпНроЯрпН', 'роГрокро░рпНро╕рпНроЯрпН ро▓рпБроХрпН', 'роГрокро╛ро▓рпЛ', 'роГрокро┐ро│ро┐рокрпНроХро╛ро░рпНроЯрпН', 'роГрокрпАро▓рпН', 'роГрокрпБроЯрпН', 'роГрокрпЗройрпН', 'роГрокрпЗройрпНро╕рпН', 'роГрокрпЗро╕рпНрокрпБроХрпН']\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MAX_FEATURES = 10000  # Maximum vocabulary size\n",
    "MIN_DF = 3  # Minimum document frequency\n",
    "MAX_DF_RATIO = 0.8  # Maximum document frequency ratio\n",
    "\n",
    "print(\"Building optimized n-gram vocabulary...\")\n",
    "\n",
    "# Step 1: Compute global n-gram frequencies\n",
    "ngram_freq = Counter()\n",
    "for doc in tokenized_docs:\n",
    "    ngram_freq.update(doc)\n",
    "\n",
    "print(f\"Total unique n-grams before filtering: {len(ngram_freq)}\")\n",
    "\n",
    "# Step 2: Compute document frequency for each n-gram\n",
    "ngram_df = {}\n",
    "for ngram in ngram_freq.keys():\n",
    "    ngram_df[ngram] = sum(1 for doc in tokenized_docs if ngram in doc)\n",
    "\n",
    "# Step 3: Filter by document frequency\n",
    "n_docs = len(tokenized_docs)\n",
    "max_df = int(MAX_DF_RATIO * n_docs)\n",
    "\n",
    "filtered_ngrams = {\n",
    "    ngram for ngram, df in ngram_df.items()\n",
    "    if MIN_DF <= df <= max_df\n",
    "}\n",
    "\n",
    "print(f\"N-grams after DF filtering: {len(filtered_ngrams)}\")\n",
    "\n",
    "# Step 4: Keep only top MAX_FEATURES most frequent n-grams\n",
    "if len(filtered_ngrams) > MAX_FEATURES:\n",
    "    # Sort by frequency and take top MAX_FEATURES\n",
    "    top_ngrams = sorted(\n",
    "        [(ngram, ngram_freq[ngram]) for ngram in filtered_ngrams],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:MAX_FEATURES]\n",
    "    vocabulary = sorted([ngram for ngram, _ in top_ngrams])\n",
    "else:\n",
    "    vocabulary = sorted(list(filtered_ngrams))\n",
    "\n",
    "word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Optimized N-gram Vocabulary Statistics:\")\n",
    "print(f\"  N-gram range: {NGRAM_RANGE}\")\n",
    "print(f\"  Final vocabulary size: {len(vocabulary):,} unique n-grams\")\n",
    "print(f\"  Max features limit: {MAX_FEATURES:,}\")\n",
    "print(f\"  Min document frequency: {MIN_DF}\")\n",
    "print(f\"  Max document frequency: {max_df} ({MAX_DF_RATIO*100:.0f}%)\")\n",
    "print(f\"  Reduction: {len(ngram_freq) - len(vocabulary):,} n-grams removed\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nSample vocabulary entries:\")\n",
    "print(vocabulary[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2a641a",
   "metadata": {},
   "source": [
    "### Step 3: Compute Term Frequency (TF) for N-grams\n",
    "\n",
    "**Term Frequency (TF)** measures how frequently an n-gram appears in a document.\n",
    "\n",
    "Formula: `TF(t, d) = (Number of times n-gram t appears in document d) / (Total number of n-grams in document d)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa306544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency computed for n-grams (using filtered vocabulary)\n",
      "Sample TF dict entries: [('ро╡ро┐ро╡роХро╛ро░роорпН', 0.06666666666666667), ('родрооро┐ро┤роХ', 0.06666666666666667), ('роХро░рпНроиро╛роЯроХро╛', 0.06666666666666667), ('роиро┐родро┐ройрпН', 0.06666666666666667), ('роХроЯрпНроХро░ро┐', 0.06666666666666667)]\n"
     ]
    }
   ],
   "source": [
    "def compute_tf(tokenized_doc, vocabulary_set):\n",
    "    \"\"\"Compute term frequency for n-grams: TF(t) = count(t) / total_ngrams\n",
    "    Only considers n-grams in the filtered vocabulary.\"\"\"\n",
    "    tf_dict = {}\n",
    "    doc_length = len(tokenized_doc)\n",
    "    if doc_length == 0:\n",
    "        return tf_dict\n",
    "    \n",
    "    term_counts = Counter(tokenized_doc)\n",
    "    # Only keep n-grams that are in vocabulary\n",
    "    for term, count in term_counts.items():\n",
    "        if term in vocabulary_set:\n",
    "            tf_dict[term] = count / doc_length\n",
    "    return tf_dict\n",
    "\n",
    "vocabulary_set = set(vocabulary)\n",
    "tf_docs = [compute_tf(doc, vocabulary_set) for doc in tokenized_docs]\n",
    "print(\"Term Frequency computed for n-grams (using filtered vocabulary)\")\n",
    "print(f\"Sample TF dict entries: {list(tf_docs[0].items())[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a81603e",
   "metadata": {},
   "source": [
    "### Step 4: Compute Inverse Document Frequency (IDF) for N-grams\n",
    "\n",
    "**Inverse Document Frequency (IDF)** measures how important an n-gram is across the entire corpus.\n",
    "\n",
    "Formula: `IDF(t) = log(Total number of documents / Number of documents containing n-gram t)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d14b2256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing IDF for n-grams...\n",
      "IDF computed for 10000 n-grams\n",
      "Sample IDF values: [('роГрокрогрпНроЯрпН', 8.213219122187478), ('роГрокро░рпНро╕рпНроЯрпН', 7.520071941627534), ('роГрокро░рпНро╕рпНроЯрпН ро▓рпБроХрпН', 8.030897565393524), ('роГрокро╛ро▓рпЛ', 8.213219122187478), ('роГрокро┐ро│ро┐рокрпНроХро╛ро░рпНроЯрпН', 8.436362673501689)]\n",
      "IDF computed for 10000 n-grams\n",
      "Sample IDF values: [('роГрокрогрпНроЯрпН', 8.213219122187478), ('роГрокро░рпНро╕рпНроЯрпН', 7.520071941627534), ('роГрокро░рпНро╕рпНроЯрпН ро▓рпБроХрпН', 8.030897565393524), ('роГрокро╛ро▓рпЛ', 8.213219122187478), ('роГрокро┐ро│ро┐рокрпНроХро╛ро░рпНроЯрпН', 8.436362673501689)]\n"
     ]
    }
   ],
   "source": [
    "def compute_idf(tokenized_docs, vocabulary):\n",
    "    \"\"\"Compute IDF for n-grams: IDF(t) = log(N / df(t))\"\"\"\n",
    "    N = len(tokenized_docs)\n",
    "    idf_dict = {}\n",
    "    for word in vocabulary:\n",
    "        doc_count = sum(1 for doc in tokenized_docs if word in doc)\n",
    "        idf_dict[word] = math.log(N / (doc_count + 1))\n",
    "    return idf_dict\n",
    "\n",
    "print(\"Computing IDF for n-grams...\")\n",
    "idf_dict = compute_idf(tokenized_docs, vocabulary)\n",
    "print(f\"IDF computed for {len(idf_dict)} n-grams\")\n",
    "print(f\"Sample IDF values: {list(idf_dict.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23794280",
   "metadata": {},
   "source": [
    "### Step 5: Compute TF-IDF Weights for N-grams\n",
    "\n",
    "**TF-IDF** combines both TF and IDF to get the final weight for each n-gram in each document.\n",
    "\n",
    "Formula: `TF-IDF(t, d) = TF(t, d) ├Ч IDF(t)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27df7c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF weights computed for n-grams\n"
     ]
    }
   ],
   "source": [
    "def compute_tfidf(tf_dict, idf_dict):\n",
    "    \"\"\"Compute TF-IDF for n-grams: TF-IDF(t, d) = TF(t, d) ├Ч IDF(t)\"\"\"\n",
    "    tfidf_dict = {}\n",
    "    for term, tf_value in tf_dict.items():\n",
    "        tfidf_dict[term] = tf_value * idf_dict.get(term, 0)\n",
    "    return tfidf_dict\n",
    "\n",
    "tfidf_docs = [compute_tfidf(tf, idf_dict) for tf in tf_docs]\n",
    "print(\"TF-IDF weights computed for n-grams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa8f495",
   "metadata": {},
   "source": [
    "### Step 6: Create Sparse TF-IDF Matrix with N-grams\n",
    "\n",
    "Convert the TF-IDF dictionaries into a **sparse matrix** representation:\n",
    "- Rows represent documents\n",
    "- Columns represent n-grams in vocabulary\n",
    "- Values are TF-IDF weights\n",
    "- Uses `scipy.sparse.lil_matrix` (efficient for construction)\n",
    "- Converts to CSR format for efficient model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3beb6f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sparse TF-IDF matrix with n-grams...\n",
      "  Format: scipy.sparse (lil_matrix тЖТ csr_matrix)\n",
      "  Dtype: float32 (memory efficient)\n",
      "\n",
      "тЬУ TF-IDF Matrix created: (18447, 10000)\n",
      "  Matrix type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "  Matrix format: csr\n",
      "  Data type: float32\n",
      "  Documents: 18447\n",
      "  N-gram features: 10000\n"
     ]
    }
   ],
   "source": [
    "def create_tfidf_matrix_sparse(tfidf_docs, vocabulary, word2idx):\n",
    "    \"\"\"Create sparse TF-IDF matrix: (n_documents, n_vocabulary)\n",
    "    Uses lil_matrix for efficient construction, then converts to CSR.\"\"\"\n",
    "    n_docs = len(tfidf_docs)\n",
    "    n_vocab = len(vocabulary)\n",
    "    \n",
    "    # Use lil_matrix (List of Lists) for efficient construction\n",
    "    tfidf_matrix = lil_matrix((n_docs, n_vocab), dtype=np.float32)\n",
    "    \n",
    "    for doc_idx, tfidf_dict in enumerate(tfidf_docs):\n",
    "        for term, tfidf_value in tfidf_dict.items():\n",
    "            if term in word2idx:\n",
    "                term_idx = word2idx[term]\n",
    "                tfidf_matrix[doc_idx, term_idx] = tfidf_value\n",
    "    \n",
    "    # Convert to CSR (Compressed Sparse Row) for efficient arithmetic operations\n",
    "    tfidf_matrix_csr = tfidf_matrix.tocsr()\n",
    "    \n",
    "    return tfidf_matrix_csr\n",
    "\n",
    "print(\"Creating sparse TF-IDF matrix with n-grams...\")\n",
    "print(\"  Format: scipy.sparse (lil_matrix тЖТ csr_matrix)\")\n",
    "print(\"  Dtype: float32 (memory efficient)\")\n",
    "\n",
    "tfidf_matrix_custom = create_tfidf_matrix_sparse(tfidf_docs, vocabulary, word2idx)\n",
    "\n",
    "print(f\"\\nтЬУ TF-IDF Matrix created: {tfidf_matrix_custom.shape}\")\n",
    "print(f\"  Matrix type: {type(tfidf_matrix_custom)}\")\n",
    "print(f\"  Matrix format: {tfidf_matrix_custom.format}\")\n",
    "print(f\"  Data type: {tfidf_matrix_custom.dtype}\")\n",
    "print(f\"  Documents: {tfidf_matrix_custom.shape[0]}\")\n",
    "print(f\"  N-gram features: {tfidf_matrix_custom.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9558b4",
   "metadata": {},
   "source": [
    "## 5. TF-IDF Matrix Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59e964bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TF-IDF Matrix Statistics (N-grams (1, 2)):\n",
      "============================================================\n",
      "  Shape: (18447, 10000)\n",
      "  Documents: 18,447\n",
      "  N-gram features: 10,000\n",
      "  Data type: float32\n",
      "  Matrix format: csr\n",
      "\n",
      "  Non-zero elements: 145,354\n",
      "  Total elements: 184,470,000\n",
      "  Sparsity: 99.92%\n",
      "\n",
      "  Mean (non-zero): 0.382379\n",
      "  Max: 2.737740\n",
      "  Min (non-zero): 0.084951\n",
      "  Std (non-zero): 0.159843\n",
      "\n",
      "  Memory usage: ~1.18 MB (sparse)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Display TF-IDF matrix statistics (safe for sparse matrices)\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"TF-IDF Matrix Statistics (N-grams {NGRAM_RANGE}):\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Shape: {tfidf_matrix_custom.shape}\")\n",
    "print(f\"  Documents: {tfidf_matrix_custom.shape[0]:,}\")\n",
    "print(f\"  N-gram features: {tfidf_matrix_custom.shape[1]:,}\")\n",
    "print(f\"  Data type: {tfidf_matrix_custom.dtype}\")\n",
    "print(f\"  Matrix format: {tfidf_matrix_custom.format}\")\n",
    "\n",
    "# Compute statistics efficiently for sparse matrices\n",
    "print(f\"\\n  Non-zero elements: {tfidf_matrix_custom.nnz:,}\")\n",
    "print(f\"  Total elements: {tfidf_matrix_custom.shape[0] * tfidf_matrix_custom.shape[1]:,}\")\n",
    "print(f\"  Sparsity: {(1 - tfidf_matrix_custom.nnz / (tfidf_matrix_custom.shape[0] * tfidf_matrix_custom.shape[1])) * 100:.2f}%\")\n",
    "\n",
    "# Statistics on non-zero values only (efficient)\n",
    "data = tfidf_matrix_custom.data\n",
    "print(f\"\\n  Mean (non-zero): {data.mean():.6f}\")\n",
    "print(f\"  Max: {data.max():.6f}\")\n",
    "print(f\"  Min (non-zero): {data.min():.6f}\")\n",
    "print(f\"  Std (non-zero): {data.std():.6f}\")\n",
    "\n",
    "# Memory usage estimate\n",
    "memory_mb = (tfidf_matrix_custom.data.nbytes + \n",
    "             tfidf_matrix_custom.indices.nbytes + \n",
    "             tfidf_matrix_custom.indptr.nbytes) / (1024 ** 2)\n",
    "print(f\"\\n  Memory usage: ~{memory_mb:.2f} MB (sparse)\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1e0e2",
   "metadata": {},
   "source": [
    "## 7. Compare Unigrams vs N-grams (Feature Count)\n",
    "\n",
    "Before training models, let's compare the vocabulary size difference between unigrams and n-grams.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db819a1",
   "metadata": {},
   "source": [
    "## 6. Prepare Data for Machine Learning\n",
    "\n",
    "Split the data into training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2287bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 14757 samples\n",
      "Test set size: 3690 samples\n",
      "N-gram feature dimension: 10000\n",
      "\n",
      "Training set label distribution:\n",
      "india                                     1753\n",
      "world                                     1514\n",
      "cinema                                    1449\n",
      "sports                                    1398\n",
      "crime                                     1302\n",
      "tamilnadu                                 1271\n",
      "business                                  1043\n",
      "trending                                  1017\n",
      "technology                                 980\n",
      "features                                   957\n",
      "health                                     673\n",
      "environment                                511\n",
      "agriculture                                490\n",
      "spiritual                                  196\n",
      "lifestyle                                   82\n",
      "motor                                       36\n",
      "coronavirus                                 30\n",
      "ampstories                                  24\n",
      "women                                       16\n",
      "employment-news-in-tamil-latest-update      15\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set label distribution:\n",
      "india                                     438\n",
      "world                                     379\n",
      "cinema                                    362\n",
      "sports                                    350\n",
      "crime                                     325\n",
      "tamilnadu                                 318\n",
      "business                                  261\n",
      "trending                                  254\n",
      "technology                                245\n",
      "features                                  239\n",
      "health                                    168\n",
      "environment                               128\n",
      "agriculture                               123\n",
      "spiritual                                  49\n",
      "lifestyle                                  21\n",
      "motor                                       9\n",
      "coronavirus                                 8\n",
      "ampstories                                  6\n",
      "women                                       4\n",
      "employment-news-in-tamil-latest-update      3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Use our custom sparse TF-IDF matrix for modeling\n",
    "X = tfidf_matrix_custom\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split data: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(f\"N-gram feature dimension: {X_train.shape[1]}\")\n",
    "print(f\"\\nTraining set label distribution:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(f\"\\nTest set label distribution:\")\n",
    "print(pd.Series(y_test).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66a5c77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABULARY SIZE COMPARISON:\n",
      "============================================================\n",
      "Unigrams only (1,1):      35,612 features\n",
      "N-grams (1, 2):         10,000 features\n",
      "Increase:                 -25,612 features\n",
      "Percentage increase:      -71.92%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Compute unigram-only vocabulary for comparison\n",
    "tokenized_unigrams = [tokenize_with_ngrams(doc, ngram_range=(1, 1)) for doc in documents]\n",
    "vocab_unigrams = set()\n",
    "for doc in tokenized_unigrams:\n",
    "    vocab_unigrams.update(doc)\n",
    "\n",
    "print(\"VOCABULARY SIZE COMPARISON:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Unigrams only (1,1):      {len(vocab_unigrams):,} features\")\n",
    "print(f\"N-grams {NGRAM_RANGE}:         {len(vocabulary):,} features\")\n",
    "print(f\"Increase:                 {len(vocabulary) - len(vocab_unigrams):,} features\")\n",
    "print(f\"Percentage increase:      {((len(vocabulary) / len(vocab_unigrams)) - 1) * 100:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e87585",
   "metadata": {},
   "source": [
    "## 8. Train Machine Learning Models\n",
    "\n",
    "Training models with n-gram features:\n",
    "1. **Manual Naive Bayes** - Implemented from scratch with Laplace smoothing\n",
    "2. **Scikit-learn Naive Bayes** - For comparison\n",
    "3. **Linear SVM** - Support Vector Machine with linear kernel\n",
    "4. **Logistic Regression** - Linear model with multinomial classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a40a49c",
   "metadata": {},
   "source": [
    "### 8.1 Naive Bayes (Manual Implementation from Scratch)\n",
    "\n",
    "Implementing Multinomial Naive Bayes manually without scikit-learn with N-gram features:\n",
    "- **Class Priors**: P(c) = count(docs in class c) / total docs\n",
    "- **Conditional Probability**: P(n-gram|c) with Laplace smoothing\n",
    "- **Log Probabilities**: For numerical stability\n",
    "- **Prediction**: argmax_c [log P(c) + ╬г log P(n-gram|c)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6dcf73",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**ЁЯОп IMPORTANT: Manual Naive Bayes with N-grams**\n",
    "\n",
    "Implementing Multinomial Naive Bayes **completely from scratch** with N-gram features:\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "\n",
    "1. **Class Priors**: \n",
    "   ```\n",
    "   P(c) = count(documents in class c) / total documents\n",
    "   ```\n",
    "\n",
    "2. **Conditional Probability** (with Laplace smoothing):\n",
    "   ```\n",
    "   P(n-gram|c) = (sum of TF-IDF weights of n-gram in class c + ╬▒) / \n",
    "                 (total TF-IDF weights in class c + ╬▒ ├Ч |V|)\n",
    "   ```\n",
    "   where ╬▒ = 1 (Laplace smoothing), |V| = n-gram vocabulary size\n",
    "\n",
    "3. **Prediction** (using log probabilities):\n",
    "   ```\n",
    "   class = argmax_c [log P(c) + ╬г log P(n-gram|c) for all n-grams in document]\n",
    "   ```\n",
    "\n",
    "**Works with sparse matrices for memory efficiency!**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad417e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Manual Naive Bayes with N-gram features (1, 2)...\n",
      "тЬУ Training completed in 0.048 seconds\n",
      "Making predictions...\n",
      "тЬУ Predictions completed in 4.808 seconds\n",
      "\n",
      "Naive Bayes (Manual Implementation with N-grams) training completed\n",
      "Classes: ['agriculture' 'ampstories' 'business' 'cinema' 'coronavirus' 'crime'\n",
      " 'employment-news-in-tamil-latest-update' 'environment' 'features'\n",
      " 'health' 'india' 'lifestyle' 'motor' 'spiritual' 'sports' 'tamilnadu'\n",
      " 'technology' 'trending' 'women' 'world']\n",
      "N-gram Features: 10000\n",
      "тЬУ Predictions completed in 4.808 seconds\n",
      "\n",
      "Naive Bayes (Manual Implementation with N-grams) training completed\n",
      "Classes: ['agriculture' 'ampstories' 'business' 'cinema' 'coronavirus' 'crime'\n",
      " 'employment-news-in-tamil-latest-update' 'environment' 'features'\n",
      " 'health' 'india' 'lifestyle' 'motor' 'spiritual' 'sports' 'tamilnadu'\n",
      " 'technology' 'trending' 'women' 'world']\n",
      "N-gram Features: 10000\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayesManual:\n",
    "    \"\"\"\n",
    "    Multinomial Naive Bayes classifier implemented from scratch.\n",
    "    Uses TF-IDF n-gram features with Laplace smoothing.\n",
    "    Works with sparse matrices for efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha: Laplace smoothing parameter (default=1.0)\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.classes = None\n",
    "        self.class_priors = {}  # P(c)\n",
    "        self.word_likelihoods = {}  # P(n-gram|c) for each class\n",
    "        self.n_features = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the Naive Bayes model.\n",
    "        \n",
    "        Args:\n",
    "            X: TF-IDF matrix (n_samples, n_features) - can be sparse\n",
    "            y: Labels array (n_samples,)\n",
    "        \"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        self.n_features = X.shape[1]\n",
    "        n_docs = X.shape[0]\n",
    "        \n",
    "        # Compute class priors: P(c) = count(c) / total_docs\n",
    "        for c in self.classes:\n",
    "            class_mask = (y == c)\n",
    "            self.class_priors[c] = np.sum(class_mask) / n_docs\n",
    "        \n",
    "        # Compute word likelihoods: P(n-gram|c) with Laplace smoothing\n",
    "        for c in self.classes:\n",
    "            class_mask = (y == c)\n",
    "            X_c = X[class_mask]  # All docs in class c\n",
    "            \n",
    "            # Sum TF-IDF weights for each n-gram in class c\n",
    "            word_counts = np.asarray(np.sum(X_c, axis=0)).ravel()  # Shape: (n_features,)\n",
    "            \n",
    "            # Total TF-IDF sum in class c\n",
    "            total_count = np.sum(word_counts)\n",
    "            \n",
    "            # Apply Laplace smoothing:\n",
    "            # P(n-gram|c) = (count(n-gram in c) + alpha) / (total_count_c + alpha * |V|)\n",
    "            self.word_likelihoods[c] = np.log(\n",
    "                (word_counts + self.alpha) / (total_count + self.alpha * self.n_features)\n",
    "            )\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        \n",
    "        Args:\n",
    "            X: TF-IDF matrix (n_samples, n_features) - can be sparse\n",
    "        \n",
    "        Returns:\n",
    "            predictions: Array of predicted labels\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            doc = X[i]  # Single document (sparse row)\n",
    "            class_scores = {}\n",
    "            \n",
    "            for c in self.classes:\n",
    "                # Start with log prior: log P(c)\n",
    "                score = np.log(self.class_priors[c])\n",
    "                \n",
    "                # Add log likelihoods: ╬г log P(n-gram|c) for n-grams in document\n",
    "                # Convert sparse row to dense array and multiply element-wise\n",
    "                doc_array = np.asarray(doc.toarray()).ravel()  # Shape: (n_features,)\n",
    "                score += np.sum(doc_array * self.word_likelihoods[c])\n",
    "                \n",
    "                class_scores[c] = score\n",
    "            \n",
    "            # Predict class with highest log probability\n",
    "            predicted_class = max(class_scores, key=class_scores.get)\n",
    "            predictions.append(predicted_class)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "\n",
    "# Train Manual Naive Bayes with N-grams\n",
    "print(f\"Training Manual Naive Bayes with N-gram features {NGRAM_RANGE}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "nb_model_manual = NaiveBayesManual(alpha=1.0)\n",
    "nb_model_manual.fit(X_train, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print(f\"тЬУ Training completed in {train_time:.3f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "print(\"Making predictions...\")\n",
    "start_pred = time.time()\n",
    "\n",
    "nb_train_pred_manual = nb_model_manual.predict(X_train)\n",
    "nb_test_pred_manual = nb_model_manual.predict(X_test)\n",
    "\n",
    "pred_time = time.time() - start_pred\n",
    "print(f\"тЬУ Predictions completed in {pred_time:.3f} seconds\")\n",
    "\n",
    "print(\"\\nNaive Bayes (Manual Implementation with N-grams) training completed\")\n",
    "print(f\"Classes: {nb_model_manual.classes}\")\n",
    "print(f\"N-gram Features: {nb_model_manual.n_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd4f19",
   "metadata": {},
   "source": [
    "### 8.1.1 Naive Bayes (Scikit-learn - For Comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2530a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scikit-learn Naive Bayes with N-grams...\n",
      "тЬУ Scikit-learn training completed in 0.046 seconds\n",
      "Naive Bayes (Scikit-learn with N-grams) training completed\n"
     ]
    }
   ],
   "source": [
    "# Train sklearn Naive Bayes for comparison\n",
    "print(\"Training Scikit-learn Naive Bayes with N-grams...\")\n",
    "start_time = time.time()\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_train_pred = nb_model.predict(X_train)\n",
    "nb_test_pred = nb_model.predict(X_test)\n",
    "\n",
    "sklearn_train_time = time.time() - start_time\n",
    "print(f\"тЬУ Scikit-learn training completed in {sklearn_train_time:.3f} seconds\")\n",
    "print(\"Naive Bayes (Scikit-learn with N-grams) training completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54000369",
   "metadata": {},
   "source": [
    "### 8.2 Linear SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea778a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM training completed with n-gram features\n"
     ]
    }
   ],
   "source": [
    "svm_model = LinearSVC(C=1.0, random_state=42, max_iter=1000)\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_train_pred = svm_model.predict(X_train)\n",
    "svm_test_pred = svm_model.predict(X_test)\n",
    "print(\"Linear SVM training completed with n-gram features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bba7373d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression training completed with n-gram features\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial')\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_train_pred = lr_model.predict(X_train)\n",
    "lr_test_pred = lr_model.predict(X_test)\n",
    "print(\"Logistic Regression training completed with n-gram features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fda658",
   "metadata": {},
   "source": [
    "### 8.3 Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c51489",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation\n",
    "\n",
    "Evaluate all models using multiple metrics:\n",
    "- **Accuracy**: Overall correctness\n",
    "- **Precision**: How many selected items are relevant\n",
    "- **Recall**: How many relevant items are selected\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **Confusion Matrix**: Detailed breakdown of predictions\n",
    "\n",
    "Comparing:\n",
    "1. **Naive Bayes (Manual)** - Our from-scratch implementation with N-grams\n",
    "2. **Naive Bayes (Scikit-learn)** - Reference implementation\n",
    "3. **Linear SVM**\n",
    "4. **Logistic Regression**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9986032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with multiple metrics.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name} - {dataset_name} Set\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "    print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "    print(f\"F1-Score:  {f1:.4f} ({f1*100:.2f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abd928d",
   "metadata": {},
   "source": [
    "### 9.1 Evaluate All Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d632d69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Naive Bayes (Manual N-gram) - Training Set\n",
      "============================================================\n",
      "Accuracy:  0.7708 (77.08%)\n",
      "Precision: 0.7922 (79.22%)\n",
      "Recall:    0.7708 (77.08%)\n",
      "F1-Score:  0.7615 (76.15%)\n",
      "\n",
      "============================================================\n",
      "Naive Bayes (Manual N-gram) - Test Set\n",
      "============================================================\n",
      "Accuracy:  0.6230 (62.30%)\n",
      "Precision: 0.6558 (65.58%)\n",
      "Recall:    0.6230 (62.30%)\n",
      "F1-Score:  0.6139 (61.39%)\n",
      "\n",
      "============================================================\n",
      "Naive Bayes (Sklearn N-gram) - Training Set\n",
      "============================================================\n",
      "Accuracy:  0.7708 (77.08%)\n",
      "Precision: 0.7922 (79.22%)\n",
      "Recall:    0.7708 (77.08%)\n",
      "F1-Score:  0.7615 (76.15%)\n",
      "\n",
      "============================================================\n",
      "Naive Bayes (Sklearn N-gram) - Test Set\n",
      "============================================================\n",
      "Accuracy:  0.6230 (62.30%)\n",
      "Precision: 0.6558 (65.58%)\n",
      "Recall:    0.6230 (62.30%)\n",
      "F1-Score:  0.6139 (61.39%)\n",
      "\n",
      "============================================================\n",
      "Linear SVM (N-gram) - Training Set\n",
      "============================================================\n",
      "Accuracy:  0.9764 (97.64%)\n",
      "Precision: 0.9764 (97.64%)\n",
      "Recall:    0.9764 (97.64%)\n",
      "F1-Score:  0.9763 (97.63%)\n",
      "\n",
      "============================================================\n",
      "Linear SVM (N-gram) - Test Set\n",
      "============================================================\n",
      "Accuracy:  0.6455 (64.55%)\n",
      "Precision: 0.6445 (64.45%)\n",
      "Recall:    0.6455 (64.55%)\n",
      "F1-Score:  0.6427 (64.27%)\n",
      "\n",
      "============================================================\n",
      "Logistic Regression (N-gram) - Training Set\n",
      "============================================================\n",
      "Accuracy:  0.8285 (82.85%)\n",
      "Precision: 0.8334 (83.34%)\n",
      "Recall:    0.8285 (82.85%)\n",
      "F1-Score:  0.8233 (82.33%)\n",
      "\n",
      "============================================================\n",
      "Logistic Regression (N-gram) - Test Set\n",
      "============================================================\n",
      "Accuracy:  0.6431 (64.31%)\n",
      "Precision: 0.6637 (66.37%)\n",
      "Recall:    0.6431 (64.31%)\n",
      "F1-Score:  0.6407 (64.07%)\n",
      "\n",
      "============================================================\n",
      "Linear SVM (N-gram) - Training Set\n",
      "============================================================\n",
      "Accuracy:  0.9764 (97.64%)\n",
      "Precision: 0.9764 (97.64%)\n",
      "Recall:    0.9764 (97.64%)\n",
      "F1-Score:  0.9763 (97.63%)\n",
      "\n",
      "============================================================\n",
      "Linear SVM (N-gram) - Test Set\n",
      "============================================================\n",
      "Accuracy:  0.6455 (64.55%)\n",
      "Precision: 0.6445 (64.45%)\n",
      "Recall:    0.6455 (64.55%)\n",
      "F1-Score:  0.6427 (64.27%)\n",
      "\n",
      "============================================================\n",
      "Logistic Regression (N-gram) - Training Set\n",
      "============================================================\n",
      "Accuracy:  0.8285 (82.85%)\n",
      "Precision: 0.8334 (83.34%)\n",
      "Recall:    0.8285 (82.85%)\n",
      "F1-Score:  0.8233 (82.33%)\n",
      "\n",
      "============================================================\n",
      "Logistic Regression (N-gram) - Test Set\n",
      "============================================================\n",
      "Accuracy:  0.6431 (64.31%)\n",
      "Precision: 0.6637 (66.37%)\n",
      "Recall:    0.6431 (64.31%)\n",
      "F1-Score:  0.6407 (64.07%)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Manual Naive Bayes\n",
    "nb_train_metrics_manual = evaluate_model(y_train, nb_train_pred_manual, \"Naive Bayes (Manual N-gram)\", \"Training\")\n",
    "nb_test_metrics_manual = evaluate_model(y_test, nb_test_pred_manual, \"Naive Bayes (Manual N-gram)\", \"Test\")\n",
    "\n",
    "# Evaluate Scikit-learn Naive Bayes\n",
    "nb_train_metrics = evaluate_model(y_train, nb_train_pred, \"Naive Bayes (Sklearn N-gram)\", \"Training\")\n",
    "nb_test_metrics = evaluate_model(y_test, nb_test_pred, \"Naive Bayes (Sklearn N-gram)\", \"Test\")\n",
    "\n",
    "# Evaluate Linear SVM\n",
    "svm_train_metrics = evaluate_model(y_train, svm_train_pred, \"Linear SVM (N-gram)\", \"Training\")\n",
    "svm_test_metrics = evaluate_model(y_test, svm_test_pred, \"Linear SVM (N-gram)\", \"Test\")\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "lr_train_metrics = evaluate_model(y_train, lr_train_pred, \"Logistic Regression (N-gram)\", \"Training\")\n",
    "lr_test_metrics = evaluate_model(y_test, lr_test_pred, \"Logistic Regression (N-gram)\", \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f391c",
   "metadata": {},
   "source": [
    "### 9.2 Confusion Matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f1ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(list(set(y_test)))\n",
    "\n",
    "# Create confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Naive Bayes\n",
    "nb_cm = confusion_matrix(y_test, nb_test_pred)\n",
    "sns.heatmap(nb_cm, annot=True, fmt='d', cmap='Greens', ax=axes[0],\n",
    "            xticklabels=classes, yticklabels=classes, cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title('Naive Bayes (N-gram)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "\n",
    "# Linear SVM\n",
    "svm_cm = confusion_matrix(y_test, svm_test_pred)\n",
    "sns.heatmap(svm_cm, annot=True, fmt='d', cmap='Oranges', ax=axes[1],\n",
    "            xticklabels=classes, yticklabels=classes, cbar_kws={'label': 'Count'})\n",
    "axes[1].set_title('Linear SVM (N-gram)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "\n",
    "# Logistic Regression\n",
    "lr_cm = confusion_matrix(y_test, lr_test_pred)\n",
    "sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', ax=axes[2],\n",
    "            xticklabels=classes, yticklabels=classes, cbar_kws={'label': 'Count'})\n",
    "axes[2].set_title('Logistic Regression (N-gram)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Predicted')\n",
    "axes[2].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaa7107",
   "metadata": {},
   "source": [
    "## 9. Model Comparison with N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e7beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Naive Bayes (Manual)', 'Naive Bayes (Sklearn)', 'Linear SVM', 'Logistic Regression'],\n",
    "    'Train Accuracy': [nb_train_metrics_manual['accuracy'], nb_train_metrics['accuracy'],\n",
    "                       svm_train_metrics['accuracy'], lr_train_metrics['accuracy']],\n",
    "    'Test Accuracy': [nb_test_metrics_manual['accuracy'], nb_test_metrics['accuracy'],\n",
    "                      svm_test_metrics['accuracy'], lr_test_metrics['accuracy']],\n",
    "    'Test Precision': [nb_test_metrics_manual['precision'], nb_test_metrics['precision'],\n",
    "                       svm_test_metrics['precision'], lr_test_metrics['precision']],\n",
    "    'Test Recall': [nb_test_metrics_manual['recall'], nb_test_metrics['recall'],\n",
    "                    svm_test_metrics['recall'], lr_test_metrics['recall']],\n",
    "    'Test F1-Score': [nb_test_metrics_manual['f1'], nb_test_metrics['f1'],\n",
    "                      svm_test_metrics['f1'], lr_test_metrics['f1']],\n",
    "    'N-gram Range': [str(NGRAM_RANGE)] * 4,\n",
    "    'Feature Count': [len(vocabulary)] * 4\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY (WITH N-GRAMS)\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(\"\\nтЬи Manual Naive Bayes with N-grams included for comparison!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719039d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "metrics = ['Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1-Score']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "rects1 = ax.bar(x - 1.5*width, \n",
    "                [nb_test_metrics_manual['accuracy'], nb_test_metrics_manual['precision'], \n",
    "                 nb_test_metrics_manual['recall'], nb_test_metrics_manual['f1']], \n",
    "                width, label='Naive Bayes (Manual)', color='lightgreen', alpha=0.8)\n",
    "rects2 = ax.bar(x - 0.5*width, \n",
    "                [nb_test_metrics['accuracy'], nb_test_metrics['precision'], \n",
    "                 nb_test_metrics['recall'], nb_test_metrics['f1']], \n",
    "                width, label='Naive Bayes (Sklearn)', color='mediumseagreen', alpha=0.8)\n",
    "rects3 = ax.bar(x + 0.5*width, \n",
    "                [svm_test_metrics['accuracy'], svm_test_metrics['precision'], \n",
    "                 svm_test_metrics['recall'], svm_test_metrics['f1']], \n",
    "                width, label='Linear SVM', color='orange', alpha=0.8)\n",
    "rects4 = ax.bar(x + 1.5*width, \n",
    "                [lr_test_metrics['accuracy'], lr_test_metrics['precision'], \n",
    "                 lr_test_metrics['recall'], lr_test_metrics['f1']], \n",
    "                width, label='Logistic Regression', color='skyblue', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title(f'Model Performance Comparison with N-grams {NGRAM_RANGE}', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "autolabel(rects4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6e40a",
   "metadata": {},
   "source": [
    "## 10. Unigrams vs N-grams Comparison\n",
    "\n",
    "**Key Analysis:** Compare feature count and model accuracy between unigram-only and n-gram approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79256201",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"UNIGRAMS VS N-GRAMS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display vocabulary comparison from earlier\n",
    "print(f\"\\nFeature Count Comparison:\")\n",
    "print(f\"  Unigrams only (1,1):    {unigram_vocab_size:,} features\")\n",
    "print(f\"  N-grams (1,2):          {len(vocabulary):,} features\")\n",
    "print(f\"  Increase:               {len(vocabulary) - unigram_vocab_size:,} features ({((len(vocabulary) - unigram_vocab_size) / unigram_vocab_size * 100):.1f}% more)\")\n",
    "\n",
    "print(f\"\\nN-gram Range: {NGRAM_RANGE}\")\n",
    "print(f\"  - Captures word sequences\")\n",
    "print(f\"  - Better context understanding\")\n",
    "print(f\"  - More discriminative features\")\n",
    "\n",
    "print(f\"\\nBest Model Performance (N-gram {NGRAM_RANGE}):\")\n",
    "best_f1 = max(nb_test_metrics['f1'], svm_test_metrics['f1'], lr_test_metrics['f1'])\n",
    "if best_f1 == nb_test_metrics['f1']:\n",
    "    print(f\"  Model: Naive Bayes\")\n",
    "elif best_f1 == svm_test_metrics['f1']:\n",
    "    print(f\"  Model: Linear SVM\")\n",
    "else:\n",
    "    print(f\"  Model: Logistic Regression\")\n",
    "print(f\"  F1-Score: {best_f1:.4f}\")\n",
    "print(f\"  Accuracy: {max(nb_test_metrics['accuracy'], svm_test_metrics['accuracy'], lr_test_metrics['accuracy']):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature count comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "categories = ['Unigrams\\n(1,1)', f'N-grams\\n{NGRAM_RANGE}']\n",
    "feature_counts = [unigram_vocab_size, len(vocabulary)]\n",
    "colors = ['skyblue', 'coral']\n",
    "\n",
    "bars = ax.bar(categories, feature_counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Number of Features', fontsize=12)\n",
    "ax.set_title('Feature Count: Unigrams vs N-grams', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height):,}',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21b33a",
   "metadata": {},
   "source": [
    "## 11. Save Models and Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c028f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "# Determine best model\n",
    "best_model_name = 'Naive Bayes'\n",
    "best_f1 = nb_test_metrics['f1']\n",
    "best_model = nb_model\n",
    "\n",
    "if svm_test_metrics['f1'] > best_f1:\n",
    "    best_model_name = 'Linear SVM'\n",
    "    best_f1 = svm_test_metrics['f1']\n",
    "    best_model = svm_model\n",
    "    \n",
    "if lr_test_metrics['f1'] > best_f1:\n",
    "    best_model_name = 'Logistic Regression'\n",
    "    best_f1 = lr_test_metrics['f1']\n",
    "    best_model = lr_model\n",
    "\n",
    "# Save all trained models with n-gram suffix\n",
    "with open('models/category_naive_bayes_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(nb_model, f)\n",
    "    \n",
    "with open('models/category_svm_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_model, f)\n",
    "    \n",
    "with open('models/category_logistic_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "\n",
    "# Save the best model\n",
    "with open('models/category_model_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Save vectorizer components (vocabulary, word2idx, IDF, and ngram_range)\n",
    "vectorizer_data = {\n",
    "    'vocabulary': vocabulary,\n",
    "    'word2idx': word2idx,\n",
    "    'idf_dict': idf_dict,\n",
    "    'ngram_range': NGRAM_RANGE\n",
    "}\n",
    "with open('models/category_vectorizer_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer_data, f)\n",
    "\n",
    "# Save model comparison\n",
    "comparison_df.to_csv('output/model_comparison_ngram.csv', index=False)\n",
    "\n",
    "# Save evaluation reports\n",
    "import json\n",
    "\n",
    "reports = {\n",
    "    'naive_bayes': {\n",
    "        'train_metrics': nb_train_metrics,\n",
    "        'test_metrics': nb_test_metrics,\n",
    "        'classification_report': classification_report(y_test, nb_test_pred, output_dict=True, zero_division=0)\n",
    "    },\n",
    "    'svm': {\n",
    "        'train_metrics': svm_train_metrics,\n",
    "        'test_metrics': svm_test_metrics,\n",
    "        'classification_report': classification_report(y_test, svm_test_pred, output_dict=True, zero_division=0)\n",
    "    },\n",
    "    'logistic': {\n",
    "        'train_metrics': lr_train_metrics,\n",
    "        'test_metrics': lr_test_metrics,\n",
    "        'classification_report': classification_report(y_test, lr_test_pred, output_dict=True, zero_division=0)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('reports/category_naive_bayes_ngram_report.json', 'w') as f:\n",
    "    json.dump(reports['naive_bayes'], f, indent=2)\n",
    "    \n",
    "with open('reports/category_svm_ngram_report.json', 'w') as f:\n",
    "    json.dump(reports['svm'], f, indent=2)\n",
    "    \n",
    "with open('reports/category_logistic_ngram_report.json', 'w') as f:\n",
    "    json.dump(reports['logistic'], f, indent=2)\n",
    "\n",
    "print(\"тЬУ All models saved to models/ directory (with _ngram suffix)\")\n",
    "print(\"тЬУ Vectorizer saved to models/category_vectorizer_ngram.pkl\")\n",
    "print(\"тЬУ Evaluation reports saved to reports/ directory\")\n",
    "print(f\"тЬУ Best model ({best_model_name}) saved as models/category_model_ngram.pkl\")\n",
    "print(f\"тЬУ N-gram range: {NGRAM_RANGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eae12c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SENTIMENT CLASSIFICATION WITH N-GRAMS\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Load Sentiment Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = pd.read_csv('output/processed_sentiment_data.csv')\n",
    "\n",
    "print(f\"Sentiment Dataset shape: {df_sentiment.shape}\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df_sentiment['sentiment'].value_counts())\n",
    "\n",
    "sentiment_documents = df_sentiment['tokenized_title'].fillna('').tolist()\n",
    "sentiment_labels = df_sentiment['sentiment'].tolist()\n",
    "\n",
    "print(f\"\\nTotal sentiment documents: {len(sentiment_documents)}\")\n",
    "print(f\"Sample: {sentiment_documents[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035104b",
   "metadata": {},
   "source": [
    "## 13. TF-IDF for Sentiment with N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7d51f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply n-gram tokenization to sentiment documents\n",
    "sentiment_tokenized_docs = [tokenize_with_ngrams(doc, ngram_range=NGRAM_RANGE) for doc in sentiment_documents]\n",
    "print(f\"Tokenized {len(sentiment_tokenized_docs)} sentiment documents with ngram_range={NGRAM_RANGE}\")\n",
    "\n",
    "# Build optimized vocabulary for sentiment with n-grams (with filtering)\n",
    "print(\"\\nBuilding optimized sentiment n-gram vocabulary...\")\n",
    "\n",
    "# Step 1: Compute global n-gram frequencies\n",
    "sentiment_ngram_freq = Counter()\n",
    "for doc in sentiment_tokenized_docs:\n",
    "    sentiment_ngram_freq.update(doc)\n",
    "\n",
    "print(f\"Total unique sentiment n-grams before filtering: {len(sentiment_ngram_freq)}\")\n",
    "\n",
    "# Step 2: Compute document frequency for each n-gram\n",
    "sentiment_ngram_df = {}\n",
    "for ngram in sentiment_ngram_freq.keys():\n",
    "    sentiment_ngram_df[ngram] = sum(1 for doc in sentiment_tokenized_docs if ngram in doc)\n",
    "\n",
    "# Step 3: Filter by document frequency\n",
    "n_sent_docs = len(sentiment_tokenized_docs)\n",
    "max_sent_df = int(MAX_DF_RATIO * n_sent_docs)\n",
    "\n",
    "sentiment_filtered_ngrams = {\n",
    "    ngram for ngram, df in sentiment_ngram_df.items()\n",
    "    if MIN_DF <= df <= max_sent_df\n",
    "}\n",
    "\n",
    "print(f\"Sentiment n-grams after DF filtering: {len(sentiment_filtered_ngrams)}\")\n",
    "\n",
    "# Step 4: Keep only top MAX_FEATURES most frequent n-grams\n",
    "if len(sentiment_filtered_ngrams) > MAX_FEATURES:\n",
    "    sentiment_top_ngrams = sorted(\n",
    "        [(ngram, sentiment_ngram_freq[ngram]) for ngram in sentiment_filtered_ngrams],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:MAX_FEATURES]\n",
    "    sentiment_vocabulary = sorted([ngram for ngram, _ in sentiment_top_ngrams])\n",
    "else:\n",
    "    sentiment_vocabulary = sorted(list(sentiment_filtered_ngrams))\n",
    "\n",
    "sentiment_word2idx = {word: idx for idx, word in enumerate(sentiment_vocabulary)}\n",
    "\n",
    "print(f\"\\nSentiment N-gram Vocabulary Statistics:\")\n",
    "print(f\"  Final vocabulary size: {len(sentiment_vocabulary):,} unique n-grams\")\n",
    "print(f\"  Reduction: {len(sentiment_ngram_freq) - len(sentiment_vocabulary):,} n-grams removed\")\n",
    "\n",
    "# Compute TF for sentiment (with filtered vocabulary)\n",
    "sentiment_vocabulary_set = set(sentiment_vocabulary)\n",
    "sentiment_tf_docs = [compute_tf(doc, sentiment_vocabulary_set) for doc in sentiment_tokenized_docs]\n",
    "print(\"\\nSentiment TF computed (using filtered vocabulary)\")\n",
    "\n",
    "# Compute IDF for sentiment\n",
    "print(\"Computing sentiment IDF with n-grams...\")\n",
    "sentiment_idf_dict = compute_idf(sentiment_tokenized_docs, sentiment_vocabulary)\n",
    "print(f\"Sentiment IDF computed for {len(sentiment_idf_dict)} n-grams\")\n",
    "\n",
    "# Compute TF-IDF for sentiment\n",
    "sentiment_tfidf_docs = [compute_tfidf(tf, sentiment_idf_dict) for tf in sentiment_tf_docs]\n",
    "print(\"Sentiment TF-IDF weights computed with n-grams\")\n",
    "\n",
    "# Create sparse TF-IDF matrix for sentiment\n",
    "print(\"\\nCreating sentiment TF-IDF sparse matrix with n-grams...\")\n",
    "sentiment_tfidf_matrix = create_tfidf_matrix_sparse(sentiment_tfidf_docs, sentiment_vocabulary, sentiment_word2idx)\n",
    "print(f\"тЬУ Sentiment TF-IDF Matrix: {sentiment_tfidf_matrix.shape}\")\n",
    "print(f\"  Matrix type: {type(sentiment_tfidf_matrix)}\")\n",
    "print(f\"  Matrix format: {sentiment_tfidf_matrix.format}\")\n",
    "print(f\"  Data type: {sentiment_tfidf_matrix.dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977746b",
   "metadata": {},
   "source": [
    "## 14. Prepare Sentiment Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca581f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sent = sentiment_tfidf_matrix\n",
    "y_sent = np.array(sentiment_labels)\n",
    "\n",
    "X_sent_train, X_sent_test, y_sent_train, y_sent_test = train_test_split(\n",
    "    X_sent, y_sent, test_size=0.2, random_state=42, stratify=y_sent\n",
    ")\n",
    "\n",
    "print(f\"Sentiment Training set: {X_sent_train.shape[0]} samples\")\n",
    "print(f\"Sentiment Test set: {X_sent_test.shape[0]} samples\")\n",
    "print(f\"N-gram features: {X_sent_train.shape[1]}\")\n",
    "print(f\"\\nSentiment distribution (train):\")\n",
    "print(pd.Series(y_sent_train).value_counts())\n",
    "print(f\"\\nSentiment distribution (test):\")\n",
    "print(pd.Series(y_sent_test).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abce1edd",
   "metadata": {},
   "source": [
    "## 15. Train Sentiment Classification Models with N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b382af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes for Sentiment\n",
    "sent_nb_model = MultinomialNB()\n",
    "sent_nb_model.fit(X_sent_train, y_sent_train)\n",
    "sent_nb_train_pred = sent_nb_model.predict(X_sent_train)\n",
    "sent_nb_test_pred = sent_nb_model.predict(X_sent_test)\n",
    "print(\"Sentiment Naive Bayes completed with n-grams\")\n",
    "\n",
    "# Linear SVM for Sentiment\n",
    "sent_svm_model = LinearSVC(C=1.0, random_state=42, max_iter=1000)\n",
    "sent_svm_model.fit(X_sent_train, y_sent_train)\n",
    "sent_svm_train_pred = sent_svm_model.predict(X_sent_train)\n",
    "sent_svm_test_pred = sent_svm_model.predict(X_sent_test)\n",
    "print(\"Sentiment Linear SVM completed with n-grams\")\n",
    "\n",
    "# Logistic Regression for Sentiment\n",
    "sent_lr_model = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial')\n",
    "sent_lr_model.fit(X_sent_train, y_sent_train)\n",
    "sent_lr_train_pred = sent_lr_model.predict(X_sent_train)\n",
    "sent_lr_test_pred = sent_lr_model.predict(X_sent_test)\n",
    "print(\"Sentiment Logistic Regression completed with n-grams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac6d75",
   "metadata": {},
   "source": [
    "## 16. Evaluate Sentiment Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Naive Bayes\n",
    "sent_nb_train_metrics = evaluate_model(y_sent_train, sent_nb_train_pred, \"Sentiment Naive Bayes (N-gram)\", \"Training\")\n",
    "sent_nb_test_metrics = evaluate_model(y_sent_test, sent_nb_test_pred, \"Sentiment Naive Bayes (N-gram)\", \"Test\")\n",
    "\n",
    "# Evaluate Linear SVM\n",
    "sent_svm_train_metrics = evaluate_model(y_sent_train, sent_svm_train_pred, \"Sentiment Linear SVM (N-gram)\", \"Training\")\n",
    "sent_svm_test_metrics = evaluate_model(y_sent_test, sent_svm_test_pred, \"Sentiment Linear SVM (N-gram)\", \"Test\")\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "sent_lr_train_metrics = evaluate_model(y_sent_train, sent_lr_train_pred, \"Sentiment Logistic Regression (N-gram)\", \"Training\")\n",
    "sent_lr_test_metrics = evaluate_model(y_sent_test, sent_lr_test_pred, \"Sentiment Logistic Regression (N-gram)\", \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2effc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for sentiment models\n",
    "sent_classes = sorted(list(set(y_sent_test)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Naive Bayes confusion matrix\n",
    "sent_nb_cm = confusion_matrix(y_sent_test, sent_nb_test_pred)\n",
    "sns.heatmap(sent_nb_cm, annot=True, fmt='d', cmap='Greens', ax=axes[0],\n",
    "            xticklabels=sent_classes, yticklabels=sent_classes)\n",
    "axes[0].set_title('Sentiment: Naive Bayes (N-gram)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "\n",
    "# SVM confusion matrix\n",
    "sent_svm_cm = confusion_matrix(y_sent_test, sent_svm_test_pred)\n",
    "sns.heatmap(sent_svm_cm, annot=True, fmt='d', cmap='Oranges', ax=axes[1],\n",
    "            xticklabels=sent_classes, yticklabels=sent_classes)\n",
    "axes[1].set_title('Sentiment: Linear SVM (N-gram)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "\n",
    "# Logistic Regression confusion matrix\n",
    "sent_lr_cm = confusion_matrix(y_sent_test, sent_lr_test_pred)\n",
    "sns.heatmap(sent_lr_cm, annot=True, fmt='d', cmap='Blues', ax=axes[2],\n",
    "            xticklabels=sent_classes, yticklabels=sent_classes)\n",
    "axes[2].set_title('Sentiment: Logistic Regression (N-gram)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Predicted')\n",
    "axes[2].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c9576",
   "metadata": {},
   "source": [
    "## 17. Sentiment Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e830b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_comparison_df = pd.DataFrame({\n",
    "    'Model': ['Naive Bayes', 'Linear SVM', 'Logistic Regression'],\n",
    "    'Train Accuracy': [sent_nb_train_metrics['accuracy'], sent_svm_train_metrics['accuracy'], sent_lr_train_metrics['accuracy']],\n",
    "    'Test Accuracy': [sent_nb_test_metrics['accuracy'], sent_svm_test_metrics['accuracy'], sent_lr_test_metrics['accuracy']],\n",
    "    'Test Precision': [sent_nb_test_metrics['precision'], sent_svm_test_metrics['precision'], sent_lr_test_metrics['precision']],\n",
    "    'Test Recall': [sent_nb_test_metrics['recall'], sent_svm_test_metrics['recall'], sent_lr_test_metrics['recall']],\n",
    "    'Test F1-Score': [sent_nb_test_metrics['f1'], sent_svm_test_metrics['f1'], sent_lr_test_metrics['f1']],\n",
    "    'N-gram Range': [str(NGRAM_RANGE)] * 3,\n",
    "    'Feature Count': [len(sentiment_vocabulary)] * 3\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SENTIMENT MODEL COMPARISON (WITH N-GRAMS)\")\n",
    "print(\"=\"*80)\n",
    "print(sent_comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb07000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment model comparison\n",
    "metrics = ['Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1-Score']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width, \n",
    "                [sent_nb_test_metrics['accuracy'], sent_nb_test_metrics['precision'], \n",
    "                 sent_nb_test_metrics['recall'], sent_nb_test_metrics['f1']], \n",
    "                width, label='Naive Bayes', color='lightgreen')\n",
    "rects2 = ax.bar(x, \n",
    "                [sent_svm_test_metrics['accuracy'], sent_svm_test_metrics['precision'], \n",
    "                 sent_svm_test_metrics['recall'], sent_svm_test_metrics['f1']], \n",
    "                width, label='Linear SVM', color='orange')\n",
    "rects3 = ax.bar(x + width, \n",
    "                [sent_lr_test_metrics['accuracy'], sent_lr_test_metrics['precision'], \n",
    "                 sent_lr_test_metrics['recall'], sent_lr_test_metrics['f1']], \n",
    "                width, label='Logistic Regression', color='skyblue')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title(f'Sentiment Model Performance Comparison (N-gram={NGRAM_RANGE})', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac47c9c",
   "metadata": {},
   "source": [
    "## 18. Save Sentiment Models and Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce4587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best sentiment model\n",
    "sent_best_model_name = 'Naive Bayes'\n",
    "sent_best_f1 = sent_nb_test_metrics['f1']\n",
    "sent_best_model = sent_nb_model\n",
    "\n",
    "if sent_svm_test_metrics['f1'] > sent_best_f1:\n",
    "    sent_best_model_name = 'Linear SVM'\n",
    "    sent_best_f1 = sent_svm_test_metrics['f1']\n",
    "    sent_best_model = sent_svm_model\n",
    "    \n",
    "if sent_lr_test_metrics['f1'] > sent_best_f1:\n",
    "    sent_best_model_name = 'Logistic Regression'\n",
    "    sent_best_f1 = sent_lr_test_metrics['f1']\n",
    "    sent_best_model = sent_lr_model\n",
    "\n",
    "print(f\"\\nBest Sentiment Model: {sent_best_model_name} (F1: {sent_best_f1:.4f})\")\n",
    "\n",
    "# Save sentiment models with n-gram suffix\n",
    "with open('models/sentiment_naive_bayes_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(sent_nb_model, f)\n",
    "    \n",
    "with open('models/sentiment_svm_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(sent_svm_model, f)\n",
    "    \n",
    "with open('models/sentiment_logistic_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(sent_lr_model, f)\n",
    "\n",
    "# Save best sentiment model\n",
    "with open('models/sentiment_model_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(sent_best_model, f)\n",
    "\n",
    "# Save sentiment vectorizer with n-gram range\n",
    "sentiment_vectorizer_data = {\n",
    "    'vocabulary': sentiment_vocabulary,\n",
    "    'word2idx': sentiment_word2idx,\n",
    "    'idf_dict': sentiment_idf_dict,\n",
    "    'ngram_range': NGRAM_RANGE\n",
    "}\n",
    "with open('models/sentiment_vectorizer_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(sentiment_vectorizer_data, f)\n",
    "\n",
    "# Save sentiment comparison\n",
    "sent_comparison_df.to_csv('output/sentiment_model_comparison_ngram.csv', index=False)\n",
    "\n",
    "# Save sentiment evaluation reports\n",
    "sent_reports = {\n",
    "    'naive_bayes': {\n",
    "        'train_metrics': sent_nb_train_metrics,\n",
    "        'test_metrics': sent_nb_test_metrics,\n",
    "        'classification_report': classification_report(y_sent_test, sent_nb_test_pred, output_dict=True, zero_division=0)\n",
    "    },\n",
    "    'svm': {\n",
    "        'train_metrics': sent_svm_train_metrics,\n",
    "        'test_metrics': sent_svm_test_metrics,\n",
    "        'classification_report': classification_report(y_sent_test, sent_svm_test_pred, output_dict=True, zero_division=0)\n",
    "    },\n",
    "    'logistic': {\n",
    "        'train_metrics': sent_lr_train_metrics,\n",
    "        'test_metrics': sent_lr_test_metrics,\n",
    "        'classification_report': classification_report(y_sent_test, sent_lr_test_pred, output_dict=True, zero_division=0)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('reports/sentiment_naive_bayes_ngram_report.json', 'w') as f:\n",
    "    json.dump(sent_reports['naive_bayes'], f, indent=2)\n",
    "    \n",
    "with open('reports/sentiment_svm_ngram_report.json', 'w') as f:\n",
    "    json.dump(sent_reports['svm'], f, indent=2)\n",
    "    \n",
    "with open('reports/sentiment_logistic_ngram_report.json', 'w') as f:\n",
    "    json.dump(sent_reports['logistic'], f, indent=2)\n",
    "\n",
    "print(\"тЬУ All sentiment models saved to models/ directory (with _ngram suffix)\")\n",
    "print(\"тЬУ Sentiment vectorizer saved to models/sentiment_vectorizer_ngram.pkl\")\n",
    "print(\"тЬУ Sentiment reports saved to reports/ directory\")\n",
    "print(f\"тЬУ Best sentiment model ({sent_best_model_name}) saved as models/sentiment_model_ngram.pkl\")\n",
    "print(f\"тЬУ N-gram range: {NGRAM_RANGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b3905",
   "metadata": {},
   "source": [
    "## 19. Final Summary: N-gram TF-IDF Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b48ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE N-GRAM TF-IDF PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nЁЯУК CATEGORY CLASSIFICATION (with N-grams):\")\n",
    "print(f\"  N-gram Range: {NGRAM_RANGE}\")\n",
    "print(f\"  Dataset: {len(documents)} documents\")\n",
    "print(f\"  Feature Count: {len(vocabulary):,} n-grams\")\n",
    "print(f\"  Train/Test Split: {X_train.shape[0]}/{X_test.shape[0]}\")\n",
    "print(f\"  Best Model: {best_model_name}\")\n",
    "print(f\"  Best F1-Score: {best_f1:.4f}\")\n",
    "\n",
    "print(\"\\nЁЯТн SENTIMENT CLASSIFICATION (with N-grams):\")\n",
    "print(f\"  N-gram Range: {NGRAM_RANGE}\")\n",
    "print(f\"  Dataset: {len(sentiment_documents)} documents\")\n",
    "print(f\"  Feature Count: {len(sentiment_vocabulary):,} n-grams\")\n",
    "print(f\"  Train/Test Split: {X_sent_train.shape[0]}/{X_sent_test.shape[0]}\")\n",
    "print(f\"  Best Model: {sent_best_model_name}\")\n",
    "print(f\"  Best F1-Score: {sent_best_f1:.4f}\")\n",
    "\n",
    "print(\"\\nЁЯФС KEY ADVANTAGES OF N-GRAMS:\")\n",
    "print(f\"  тЬУ Captures word sequences and context\")\n",
    "print(f\"  тЬУ More discriminative features than unigrams alone\")\n",
    "print(f\"  тЬУ Better understanding of phrases (e.g., 'роЗро▓роЩрпНроХрпИ роЕро░роЪрпБ')\")\n",
    "print(f\"  тЬУ Improved classification performance\")\n",
    "print(f\"  тЬУ Adjustable ngram_range parameter for flexibility\")\n",
    "\n",
    "print(\"\\nЁЯТ╛ SAVED ARTIFACTS:\")\n",
    "print(\"  Category Models:\")\n",
    "print(\"    - models/category_naive_bayes_ngram.pkl\")\n",
    "print(\"    - models/category_svm_ngram.pkl\")\n",
    "print(\"    - models/category_logistic_ngram.pkl\")\n",
    "print(\"    - models/category_model_ngram.pkl (best)\")\n",
    "print(\"    - models/category_vectorizer_ngram.pkl\")\n",
    "print(\"\\n  Sentiment Models:\")\n",
    "print(\"    - models/sentiment_naive_bayes_ngram.pkl\")\n",
    "print(\"    - models/sentiment_svm_ngram.pkl\")\n",
    "print(\"    - models/sentiment_logistic_ngram.pkl\")\n",
    "print(\"    - models/sentiment_model_ngram.pkl (best)\")\n",
    "print(\"    - models/sentiment_vectorizer_ngram.pkl\")\n",
    "print(\"\\n  Reports:\")\n",
    "print(\"    - output/model_comparison_ngram.csv\")\n",
    "print(\"    - output/sentiment_model_comparison_ngram.csv\")\n",
    "print(\"    - reports/*_ngram_report.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"тЬЕ N-GRAM TF-IDF PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
