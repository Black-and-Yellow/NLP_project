{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a204add2",
   "metadata": {},
   "source": [
    "# Tamil News Title Tokenizer\n",
    "\n",
    "This notebook contains a CSV-based preprocessing pipeline for Tamil news title classification.\n",
    "Cells are laid out as: imports, tokenizer class, resource creation, initialization, and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1e505a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d64e1846",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TamilNewsTokenizer:\n",
    "    \n",
    "    \n",
    "    def __init__(self, stopwords_file: str, suffixes_file: str):\n",
    "       \n",
    "        self.stopwords = set()\n",
    "        self.suffixes = {}\n",
    "        \n",
    "        # Load stopwords\n",
    "        self._load_stopwords(stopwords_file)\n",
    "        \n",
    "        # Load suffixes\n",
    "        self._load_suffixes(suffixes_file)\n",
    "        \n",
    "        # for greedy matching\n",
    "        self.sorted_suffixes = sorted(self.suffixes.keys(), key=len, reverse=True)\n",
    "        \n",
    "        print(f\"Tokenizer initialized\")\n",
    "        print(f\"  Stopwords: {len(self.stopwords)}\")\n",
    "        print(f\"  Suffixes: {len(self.suffixes)}\")\n",
    "    \n",
    "    \n",
    "    def _load_stopwords(self, filepath: str):\n",
    "        \"\"\"Load stopwords from file (one per line).\"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"Stopwords file not found: {filepath}\")\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                self.stopwords = {line.strip() for line in f if line.strip() and not line.startswith('#')}\n",
    "            print(f\"Loaded {len(self.stopwords)} stopwords from {filepath}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading stopwords: {e}\")\n",
    "    \n",
    "    \n",
    "    def _load_suffixes(self, filepath: str):\n",
    "        \"\"\"Load suffixes from CSV file (format: suffix).\"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"Suffixes file not found: {filepath}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(filepath, encoding='utf-8')            \n",
    "            if 'suffix' not in df.columns or 'meaning' not in df.columns:\n",
    "                raise ValueError(\"Suffixes CSV must have 'suffix' and 'meaning' columns\")\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                suffix = str(row['suffix']).strip()\n",
    "                meaning = str(row['meaning']).strip()\n",
    "                \n",
    "                if suffix and meaning and not suffix.startswith('#'):\n",
    "                    self.suffixes[suffix] = meaning\n",
    "            \n",
    "            print(f\"Loaded {len(self.suffixes)} suffixes from {filepath}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading suffixes: {e}\")\n",
    "    \n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean text for news title classification.\n",
    "        - Remove digits and English letters\n",
    "        - Remove punctuation except Tamil punctuation\n",
    "        - Normalize whitespace\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text)\n",
    "        \n",
    "        text = re.sub(r'[a-zA-Z]+', '', text)\n",
    "        \n",
    "        text = re.sub(r'[0-9௦-௯]+', '', text)\n",
    "        \n",
    "        text = re.sub(r'[!\\\"#$%&\\'()*+,\\-./:;<=>?@\\[\\\\\\\\\\]^_`{|}~…–—]', ' ', text)\n",
    "        \n",
    "        text = re.sub(r'[₹$€£¥●○■□★☆♦♥♠♣]', '', text)\n",
    "        \n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def word_tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize text into words.\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "        return [w.strip() for w in text.split() if w.strip()]\n",
    "    \n",
    "    \n",
    "    def remove_stopwords(self, words: List[str]) -> List[str]:\n",
    "        \"\"\"Remove Tamil stopwords.\"\"\"\n",
    "        return [w for w in words if w not in self.stopwords]\n",
    "    \n",
    "    \n",
    "    def split_morphemes(self, word: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split word into morphemes (root + suffixes).\n",
    "        Returns flat list of morphemes for easy joining.\n",
    "        \"\"\"\n",
    "        if len(word) <= 2:\n",
    "            return [word]\n",
    "        \n",
    "        morphemes = []\n",
    "        remaining = word\n",
    "        \n",
    "        # Extract suffixes iteratively\n",
    "        while len(remaining) > 2:\n",
    "            suffix_found = False\n",
    "            \n",
    "            for suffix in self.sorted_suffixes:\n",
    "                if remaining.endswith(suffix):\n",
    "                    root = remaining[:-len(suffix)]\n",
    "                    if len(root) >= 2:\n",
    "                        morphemes.append(suffix)\n",
    "                        remaining = root\n",
    "                        suffix_found = True\n",
    "                        break\n",
    "            \n",
    "            if not suffix_found:\n",
    "                break\n",
    "        \n",
    "        morphemes.insert(0, remaining)\n",
    "        \n",
    "        return morphemes\n",
    "    \n",
    "    def remove_suffixes(self, words: List[str]) -> List[str]:\n",
    "        \"\"\"Remove Tamil suffixes.\"\"\"\n",
    "        return [w for w in words if w not in self.suffixes]\n",
    "\n",
    "\n",
    "    def process_title(self, title: str, remove_stops: bool = True, remove_suffixes: bool = False,\n",
    "                      split_morph: bool = True) -> str:\n",
    "        # Clean\n",
    "        cleaned = self.clean_text(title)\n",
    "        \n",
    "        # Tokenize\n",
    "        words = self.word_tokenize(cleaned)\n",
    "        \n",
    "        # Remove stopwords if requested\n",
    "        if remove_stops:\n",
    "            words = self.remove_stopwords(words)\n",
    "        \n",
    "        # Split morphemes if requested\n",
    "        if split_morph:\n",
    "            all_morphemes = []\n",
    "            for word in words:\n",
    "                morphemes = self.split_morphemes(word)\n",
    "                all_morphemes.extend(morphemes)\n",
    "            words = all_morphemes\n",
    "        else:\n",
    "            # Remove suffixes\n",
    "            if remove_suffixes:\n",
    "                words = self.remove_suffixes(words)\n",
    "            return ' '.join(words)\n",
    "\n",
    "        if remove_suffixes:\n",
    "            words = self.remove_suffixes(words)\n",
    "\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    \n",
    "    def process_csv(self, input_csv: str, output_csv: str, \n",
    "                    title_column: str = 'title',\n",
    "                    label_column: str = None,\n",
    "                    remove_stopwords: bool = True,\n",
    "                    remove_suffixes: bool = False,\n",
    "                    split_morphemes: bool = True):\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Tamil News Title Preprocessing\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Read CSV\n",
    "        print(f\"\\n[1] Reading CSV: {input_csv}\")\n",
    "        df = pd.read_csv(input_csv, encoding='utf-8', sep =',')\n",
    "        print(f\"  ✓ Loaded {len(df)} rows\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Check if title column exists\n",
    "        if title_column not in df.columns:\n",
    "            print(f\"  ✗ Error: Column '{title_column}' not found!\")\n",
    "            print(f\"  Available columns: {list(df.columns)}\")\n",
    "            return\n",
    "        \n",
    "        # Process titles\n",
    "        print(f\"\\n[2] Processing titles...\")\n",
    "        print(f\"  Remove stopwords: {remove_stopwords}\")\n",
    "        print(f\"  Remove suffixes: {remove_suffixes}\")\n",
    "        print(f\"  Split morphemes: {split_morphemes}\")\n",
    "        \n",
    "        # Create new columns\n",
    "        df['cleaned_title'] = df[title_column].apply(\n",
    "            lambda x: self.clean_text(x)\n",
    "        )\n",
    "        \n",
    "        df['tokenized_title'] = df['cleaned_title'].apply(\n",
    "            lambda x: ' '.join(self.word_tokenize(x))\n",
    "        )\n",
    "        \n",
    "        df['processed_title'] = df[title_column].apply(\n",
    "            lambda x: self.process_title(x, remove_stopwords, remove_suffixes, split_morphemes)\n",
    "        )\n",
    "        \n",
    "        # Calculate statistics\n",
    "        print(f\"\\n[3] Statistics:\")\n",
    "        original_words = df[title_column].apply(\n",
    "            lambda x: len(self.word_tokenize(self.clean_text(x)))\n",
    "        ).sum()\n",
    "        \n",
    "        processed_tokens = df['processed_title'].apply(\n",
    "            lambda x: len(x.split()) if x else 0\n",
    "        ).sum()\n",
    "        \n",
    "        print(f\"  Original word count: {original_words}\")\n",
    "        print(f\"  Processed token count: {processed_tokens}\")\n",
    "        \n",
    "        avg_original = df[title_column].apply(\n",
    "            lambda x: len(self.word_tokenize(self.clean_text(x)))\n",
    "        ).mean()\n",
    "        \n",
    "        avg_processed = df['processed_title'].apply(\n",
    "            lambda x: len(x.split()) if x else 0\n",
    "        ).mean()\n",
    "        \n",
    "        print(f\"  Avg words per title (original): {avg_original:.2f}\")\n",
    "        print(f\"  Avg tokens per title (processed): {avg_processed:.2f}\")\n",
    "        \n",
    "        empty_count = (df['processed_title'] == '').sum()\n",
    "        if empty_count > 0:\n",
    "            print(f\"  ⚠ Warning: {empty_count} titles became empty after processing\")\n",
    "            df = df[df['processed_title'] != '']\n",
    "        \n",
    "        print(f\"\\n[4] Saving to: {output_csv}\")\n",
    "\n",
    "        output_cols = ['processed_title']\n",
    "        if label_column and label_column in df.columns:\n",
    "            output_cols.insert(0, label_column)\n",
    "        \n",
    "        output_cols_full = output_cols + ['cleaned_title', 'tokenized_title', title_column]\n",
    "        \n",
    "        df[output_cols_full].to_csv(output_csv, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"  ✓ Saved {len(df)} rows\")\n",
    "        print(f\"  Columns: {output_cols_full}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"✓ Processing Complete!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ae1609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating resource files...\n",
      "\n",
      "✓ Created: resources/stopwords.txt\n",
      "✓ Created: resources/suffixes.csv\n",
      "\n",
      "✓ Resource files created successfully!\n",
      "  You can now edit these files to add/remove stopwords and suffixes\n"
     ]
    }
   ],
   "source": [
    "# def create_resource_files():\n",
    "#     \"\"\"\n",
    "#     Create sample resource files (stopwords and suffixes).\n",
    "#     Run this cell only once to create template files.\n",
    "#     \"\"\"\n",
    "#     print(\"Creating resource files...\\n\")\n",
    "    \n",
    "#     # Create resources directory\n",
    "#     os.makedirs('resources', exist_ok=True)\n",
    "    \n",
    "#     # Create stopwords file\n",
    "#     stopwords = \"\"\"# Tamil Stopwords for News Classification\n",
    "# # Lines starting with # are comments and will be ignored\n",
    "\n",
    "# # Pronouns and demonstratives\n",
    "# ஆக\n",
    "# ஆகவே\n",
    "# ஆதலால்\n",
    "# ஆனால்\n",
    "# என்\n",
    "# என்ற\n",
    "# என்று\n",
    "# இது\n",
    "# இதை\n",
    "# இந்த\n",
    "# இவை\n",
    "# இவர்\n",
    "# இல்லை\n",
    "# உள்ள\n",
    "# உள்ளது\n",
    "# உள்ளன\n",
    "# என\n",
    "# எனவே\n",
    "# எனில்\n",
    "# ஒரு\n",
    "# ஒன்று\n",
    "# கூட\n",
    "# தான்\n",
    "# நான்\n",
    "# நாம்\n",
    "# நீ\n",
    "# நீங்கள்\n",
    "# பல\n",
    "# பற்றி\n",
    "# மற்றும்\n",
    "# முதல்\n",
    "# வேண்டும்\n",
    "# அவர்\n",
    "# அவன்\n",
    "# அவள்\n",
    "# அது\n",
    "# அந்த\n",
    "# போல\n",
    "# போன்ற\n",
    "# மட்டும்\n",
    "# மிக\n",
    "# வரை\n",
    "# ஆம்\n",
    "# ஏன்\n",
    "# எப்படி\n",
    "# எங்கே\n",
    "# எப்போது\n",
    "# எது\n",
    "# எவர்\n",
    "# சில\n",
    "# பின்\n",
    "# முன்\n",
    "# மேல்\n",
    "# கீழ்\n",
    "# உடன்\n",
    "\n",
    "# # Additional common words\n",
    "# என்பது\n",
    "# என்பதை\n",
    "# என்பதால்\n",
    "# இருந்து\n",
    "# இருக்கும்\n",
    "# இருக்கிறது\n",
    "# செய்து\n",
    "# செய்யும்\n",
    "# செய்கிறது\n",
    "# உடைய\n",
    "# உள்ளிட்ட\n",
    "# தொடர்பான\n",
    "# சார்ந்த\n",
    "# வந்த\n",
    "# வரும்\n",
    "# கொண்ட\n",
    "# கொண்டு\n",
    "# முடியும்\n",
    "# முடிந்த\n",
    "# பெற்ற\n",
    "# பெறும்\n",
    "\n",
    "# # Time-related stopwords (optional - remove if needed for news)\n",
    "# இன்று\n",
    "# நேற்று\n",
    "# இனி\n",
    "# பின்னர்\n",
    "# தற்போது\n",
    "# அண்மையில்\n",
    "# சமீபத்தில்\n",
    "# எதிர்வரும்\n",
    "# மேலும்\n",
    "# குறிப்பாக\n",
    "# முக்கியமாக\n",
    "# \"\"\"\n",
    "    \n",
    "#     with open('resources/stopwords.txt', 'w', encoding='utf-8') as f:\n",
    "#         f.write(stopwords)\n",
    "#     print(\"✓ Created: resources/stopwords.txt\")\n",
    "    \n",
    "#     # Create suffixes CSV\n",
    "#     suffixes_data = \"\"\"suffix,meaning\n",
    "# ஐ,accusative\n",
    "# ஆல்,instrumental\n",
    "# ஆன்,genitive\n",
    "# இல்,locative\n",
    "# இலிருந்து,ablative\n",
    "# இன்,genitive\n",
    "# உக்கு,dative\n",
    "# உடன்,sociative\n",
    "# ஓடு,sociative\n",
    "# கு,dative\n",
    "# க்கு,dative\n",
    "# அது,demonstrative\n",
    "# எனில்,conditional\n",
    "# கள்,plural\n",
    "# களை,plural_acc\n",
    "# களில்,plural_loc\n",
    "# களுக்கு,plural_dat\n",
    "# களால்,plural_inst\n",
    "# களின்,plural_gen\n",
    "# களோடு,plural_soc\n",
    "# களுடன்,plural_soc\n",
    "# கிறது,pres_neut\n",
    "# கிறான்,pres_masc\n",
    "# கிறாள்,pres_fem\n",
    "# கிறார்,pres_hon\n",
    "# கிறேன்,pres_1st\n",
    "# கிறாய்,pres_2nd\n",
    "# கிறீர்,pres_2nd_hon\n",
    "# கிறோம்,pres_1st_pl\n",
    "# கின்றது,pres_neut_formal\n",
    "# கின்றான்,pres_masc_formal\n",
    "# கின்றாள்,pres_fem_formal\n",
    "# கின்றேன்,pres_1st_formal\n",
    "# கின்றார்,pres_hon_formal\n",
    "# கின்றனர்,pres_pl_formal\n",
    "# ந்தது,past_neut\n",
    "# ந்தான்,past_masc\n",
    "# ந்தாள்,past_fem\n",
    "# ந்தார்,past_hon\n",
    "# ந்தேன்,past_1st\n",
    "# ந்தாய்,past_2nd\n",
    "# ந்தோம்,past_1st_pl\n",
    "# ந்தனர்,past_pl\n",
    "# த்தது,past_neut_alt\n",
    "# த்தான்,past_masc_alt\n",
    "# த்தாள்,past_fem_alt\n",
    "# த்தார்,past_hon_alt\n",
    "# த்தேன்,past_1st_alt\n",
    "# ட்டது,past_neut_alt2\n",
    "# ட்டான்,past_masc_alt2\n",
    "# ட்டார்,past_hon_alt2\n",
    "# வான்,fut_masc\n",
    "# வாள்,fut_fem\n",
    "# வார்,fut_hon\n",
    "# வேன்,fut_1st\n",
    "# வாய்,fut_2nd\n",
    "# வோம்,fut_1st_pl\n",
    "# வார்கள்,fut_pl\n",
    "# ப்பான்,fut_masc_alt\n",
    "# ப்பாள்,fut_fem_alt\n",
    "# ப்பார்,fut_hon_alt\n",
    "# ப்பேன்,fut_1st_alt\n",
    "# கிற,pres_part\n",
    "# கின்ற,pres_part_formal\n",
    "# ந்த,past_part\n",
    "# த்த,past_part_alt\n",
    "# ட்ட,past_part_alt2\n",
    "# உம்,part_conj\n",
    "# ஆமல்,neg_part\n",
    "# ஆது,neg_part_alt\n",
    "# ஆன,adj\n",
    "# ஆனது,rel_neut\n",
    "# ஆனவர்,rel_person\n",
    "# ஆனவை,rel_pl\n",
    "# என்று,quotative\n",
    "# இடம்,place\n",
    "# படி,manner\n",
    "# போல,like\n",
    "# போது,time\n",
    "# பின்,after\n",
    "# முன்,before\n",
    "# மட்டும்,only\n",
    "# வரை,until\n",
    "# ஆக,as\n",
    "# ஆகவே,therefore\n",
    "# ஆகிய,conjunction\n",
    "# ஆகியோர்,conjunction_person\n",
    "# ஆகியவை,conjunction_things\n",
    "# ஆகியன,conjunction_neuter\n",
    "# எனும்,called\n",
    "# எனப்படும்,called_passive\n",
    "# உடைய,possessive\n",
    "# மிக்க,superlative\n",
    "# உள்ளிட்ட,including\n",
    "# சார்ந்த,related\n",
    "# தொடர்பான,related_to\n",
    "# பெற்ற,obtained\n",
    "# கொண்ட,having\n",
    "# செய்த,did\n",
    "# செய்யும்,will_do\n",
    "# செய்யப்பட்ட,done_passive\n",
    "# செய்யப்படும்,being_done\n",
    "# அளித்த,gave\n",
    "# அளித்து,giving\n",
    "# அளிக்கப்பட்ட,given_passive\n",
    "# வழங்கிய,provided\n",
    "# வழங்கும்,providing\n",
    "# ஏற்பட்ட,occurred\n",
    "# ஏற்படும்,will_occur\n",
    "# நடந்த,happened\n",
    "# நடக்கும்,will_happen\n",
    "# நடைபெற்ற,took_place\n",
    "# நடைபெறும்,will_take_place\n",
    "# கூறிய,said\n",
    "# கூறும்,saying\n",
    "# தெரிவித்த,informed\n",
    "# தெரிவிக்கும்,informing\n",
    "# அறிவித்த,announced\n",
    "# அறிவிக்கும்,announcing\n",
    "# வந்த,came\n",
    "# வரும்,coming\n",
    "# சென்ற,went\n",
    "# செல்லும்,going\n",
    "# இருந்த,was\n",
    "# இருக்கும்,will_be\n",
    "# உள்ள,having_present\n",
    "# தொடங்கிய,started\n",
    "# தொடங்கும்,starting\n",
    "# முடிந்த,finished\n",
    "# முடியும்,will_finish\n",
    "# கண்ட,saw\n",
    "# காணும்,seeing\n",
    "# ஆன,became\n",
    "# ஆகும்,becoming\n",
    "# \"\"\"\n",
    "    \n",
    "#     with open('resources/suffixes.csv', 'w', encoding='utf-8') as f:\n",
    "#         f.write(suffixes_data)\n",
    "#     print(\"✓ Created: resources/suffixes.csv\")\n",
    "    \n",
    "#     print(\"\\n✓ Resource files created successfully!\")\n",
    "#     print(\"  You can now edit these files to add/remove stopwords and suffixes\")\n",
    "\n",
    "\n",
    "# # Run this function once to create template files\n",
    "# create_resource_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7be784c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 127 stopwords from resources/stopwords.txt\n",
      "Loaded 133 suffixes from resources/suffixes.csv\n",
      "Tokenizer initialized\n",
      "  Stopwords: 127\n",
      "  Suffixes: 133\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = TamilNewsTokenizer(\n",
    "    stopwords_file='resources/stopwords.txt',\n",
    "    suffixes_file='resources/suffixes.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "780dd5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (18453, 3)\n",
      "\n",
      "Columns: ['title', 'extra', 'category']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>extra</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>மேகதாது விவகாரம்: தமிழக, கர்நாடகா முதலமைச்சர்க...</td>\n",
       "      <td>https://www.puthiyathalaimurai.com/tamilnadu/t...</td>\n",
       "      <td>தமிழ்நாடு</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>பந்துவீச்சாளர்கள் ஐபிஎல் விளையாடலாமா? - எதிரெத...</td>\n",
       "      <td>https://www.puthiyathalaimurai.com/sports/ms-d...</td>\n",
       "      <td>விளையாட்டு</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>கனமழை எச்சரிக்கை | நாளை பள்ளி, கல்லூரிகளுக்கு ...</td>\n",
       "      <td>https://www.puthiyathalaimurai.com/tamilnadu/t...</td>\n",
       "      <td>தமிழ்நாடு</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது - விஜய்க...</td>\n",
       "      <td>https://www.puthiyathalaimurai.com/tamilnadu/r...</td>\n",
       "      <td>தமிழ்நாடு</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ஆழ்ந்த காற்றழுத்த தாழ்வுப்பகுதி எதிரொலி.. 8 மா...</td>\n",
       "      <td>https://www.puthiyathalaimurai.com/tamilnadu/d...</td>\n",
       "      <td>தமிழ்நாடு</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  மேகதாது விவகாரம்: தமிழக, கர்நாடகா முதலமைச்சர்க...   \n",
       "1  பந்துவீச்சாளர்கள் ஐபிஎல் விளையாடலாமா? - எதிரெத...   \n",
       "2  கனமழை எச்சரிக்கை | நாளை பள்ளி, கல்லூரிகளுக்கு ...   \n",
       "3  தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது - விஜய்க...   \n",
       "4  ஆழ்ந்த காற்றழுத்த தாழ்வுப்பகுதி எதிரொலி.. 8 மா...   \n",
       "\n",
       "                                               extra    category  \n",
       "0  https://www.puthiyathalaimurai.com/tamilnadu/t...   தமிழ்நாடு  \n",
       "1  https://www.puthiyathalaimurai.com/sports/ms-d...  விளையாட்டு  \n",
       "2  https://www.puthiyathalaimurai.com/tamilnadu/t...   தமிழ்நாடு  \n",
       "3  https://www.puthiyathalaimurai.com/tamilnadu/r...   தமிழ்நாடு  \n",
       "4  https://www.puthiyathalaimurai.com/tamilnadu/d...   தமிழ்நாடு  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset\n",
    "from pathlib import Path\n",
    "path = Path('dataset/data.csv')\n",
    "df = pd.read_csv(path, encoding=\"utf-8\", sep=\",\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4cb9c00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Tamil News Title Preprocessing\n",
      "============================================================\n",
      "\n",
      "[1] Reading CSV: dataset/data.csv\n",
      "  ✓ Loaded 18453 rows\n",
      "  Columns: ['title', 'extra', 'category']\n",
      "\n",
      "[2] Processing titles...\n",
      "  Remove stopwords: True\n",
      "  Remove suffixes: True\n",
      "  Split morphemes: True\n",
      "\n",
      "[3] Statistics:\n",
      "  Original word count: 159562\n",
      "  Processed token count: 148735\n",
      "  Avg words per title (original): 8.65\n",
      "  Avg tokens per title (processed): 8.06\n",
      "  ⚠ Warning: 5 titles became empty after processing\n",
      "\n",
      "[4] Saving to: output/processed_data.csv\n",
      "  ✓ Saved 18448 rows\n",
      "  Columns: ['category', 'processed_title', 'cleaned_title', 'tokenized_title', 'title']\n",
      "\n",
      "============================================================\n",
      "✓ Processing Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "processed_df = tokenizer.process_csv(\n",
    "    input_csv='dataset/data.csv',\n",
    "    output_csv='output/processed_data.csv',\n",
    "    title_column='title',           \n",
    "    label_column='category',        \n",
    "    remove_stopwords=True,\n",
    "    remove_suffixes=True,\n",
    "    split_morphemes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aacc90d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed Data Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_title</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>processed_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>மேகதாது விவகாரம்: தமிழக, கர்நாடகா முதலமைச்சர்க...</td>\n",
       "      <td>மேகதாது விவகாரம் தமிழக கர்நாடகா முதலமைச்சர்களு...</td>\n",
       "      <td>மேகதாது விவகாரம் தமிழக கர்நாடகா முதலமைச்சர்களு...</td>\n",
       "      <td>மேகதாது விவகாரம் தமிழக கர்நாடகா முதலமைச்சர் நி...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>பந்துவீச்சாளர்கள் ஐபிஎல் விளையாடலாமா? - எதிரெத...</td>\n",
       "      <td>பந்துவீச்சாளர்கள் ஐபிஎல் விளையாடலாமா எதிரெதிர்...</td>\n",
       "      <td>பந்துவீச்சாளர்கள் ஐபிஎல் விளையாடலாமா எதிரெதிர்...</td>\n",
       "      <td>பந்துவீச்சாளர் ஐபிஎல் விளையாடலாமா எதிரெதிர் கர...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>கனமழை எச்சரிக்கை | நாளை பள்ளி, கல்லூரிகளுக்கு ...</td>\n",
       "      <td>கனமழை எச்சரிக்கை நாளை பள்ளி கல்லூரிகளுக்கு விட...</td>\n",
       "      <td>கனமழை எச்சரிக்கை நாளை பள்ளி கல்லூரிகளுக்கு விட...</td>\n",
       "      <td>கனமழை எச்சரிக்கை நாளை பள்ளி கல்லூரி விடுமுறை எ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது - விஜய்க...</td>\n",
       "      <td>தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது விஜய்க்க...</td>\n",
       "      <td>தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது விஜய்க்க...</td>\n",
       "      <td>தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது விஜய் ஆர...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ஆழ்ந்த காற்றழுத்த தாழ்வுப்பகுதி எதிரொலி.. 8 மா...</td>\n",
       "      <td>ஆழ்ந்த காற்றழுத்த தாழ்வுப்பகுதி எதிரொலி மாவட்ட...</td>\n",
       "      <td>ஆழ்ந்த காற்றழுத்த தாழ்வுப்பகுதி எதிரொலி மாவட்ட...</td>\n",
       "      <td>ஆழ் காற்றழு தாழ்வுப்பகுதி எதிரொலி மாவட்டங் அலர்ட்</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>உருவானது புதிய காற்றழுத்த தாழ்வுப்பகுதி.. எங்க...</td>\n",
       "      <td>உருவானது புதிய காற்றழுத்த தாழ்வுப்பகுதி எங்கெல...</td>\n",
       "      <td>உருவானது புதிய காற்றழுத்த தாழ்வுப்பகுதி எங்கெல...</td>\n",
       "      <td>உருவானது புதிய காற்றழு தாழ்வுப்பகுதி எங்கெல்லா...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>பாமகவை இழுக்க முயற்சிக்கும் NDA: கறார் காட்டும...</td>\n",
       "      <td>பாமகவை இழுக்க முயற்சிக்கும் கறார் காட்டும் ராம...</td>\n",
       "      <td>பாமகவை இழுக்க முயற்சிக்கும் கறார் காட்டும் ராம...</td>\n",
       "      <td>பாமகவை இழுக்க முயற்சிக்கும் கறார் காட்டும் ராம...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>வங்கக்கடலில் காற்றழுத்த தாழ்வுப்பகுதி: தமிழகம்...</td>\n",
       "      <td>வங்கக்கடலில் காற்றழுத்த தாழ்வுப்பகுதி தமிழகம் ...</td>\n",
       "      <td>வங்கக்கடலில் காற்றழுத்த தாழ்வுப்பகுதி தமிழகம் ...</td>\n",
       "      <td>வங்கக்கடலில் காற்றழு தாழ்வுப்பகுதி தமிழகம் முழ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>அரபிக்கடலில் காற்றழுத்த தாழ்வு\\r\\nமண்டலம்.. நா...</td>\n",
       "      <td>அரபிக்கடலில் காற்றழுத்த தாழ்வு மண்டலம் நாளை மா...</td>\n",
       "      <td>அரபிக்கடலில் காற்றழுத்த தாழ்வு மண்டலம் நாளை மா...</td>\n",
       "      <td>அரபிக்கடலில் காற்றழு தாழ்வு மண்டலம் நாளை மாவட்...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>அனிமேஷன் யுத்தம் | இணையத்தில் மல்லுக்கட்டும் த...</td>\n",
       "      <td>அனிமேஷன் யுத்தம் இணையத்தில் மல்லுக்கட்டும் திம...</td>\n",
       "      <td>அனிமேஷன் யுத்தம் இணையத்தில் மல்லுக்கட்டும் திம...</td>\n",
       "      <td>அனிமேஷன் யுத்தம் இணையத்தில் மல்லுக்கட்டும் திம...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  மேகதாது விவகாரம்: தமிழக, கர்நாடகா முதலமைச்சர்க...   \n",
       "1  பந்துவீச்சாளர்கள் ஐபிஎல் விளையாடலாமா? - எதிரெத...   \n",
       "2  கனமழை எச்சரிக்கை | நாளை பள்ளி, கல்லூரிகளுக்கு ...   \n",
       "3  தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது - விஜய்க...   \n",
       "4  ஆழ்ந்த காற்றழுத்த தாழ்வுப்பகுதி எதிரொலி.. 8 மா...   \n",
       "5  உருவானது புதிய காற்றழுத்த தாழ்வுப்பகுதி.. எங்க...   \n",
       "6  பாமகவை இழுக்க முயற்சிக்கும் NDA: கறார் காட்டும...   \n",
       "7  வங்கக்கடலில் காற்றழுத்த தாழ்வுப்பகுதி: தமிழகம்...   \n",
       "8  அரபிக்கடலில் காற்றழுத்த தாழ்வு\\r\\nமண்டலம்.. நா...   \n",
       "9  அனிமேஷன் யுத்தம் | இணையத்தில் மல்லுக்கட்டும் த...   \n",
       "\n",
       "                                       cleaned_title  \\\n",
       "0  மேகதாது விவகாரம் தமிழக கர்நாடகா முதலமைச்சர்களு...   \n",
       "1  பந்துவீச்சாளர்கள் ஐபிஎல் விளையாடலாமா எதிரெதிர்...   \n",
       "2  கனமழை எச்சரிக்கை நாளை பள்ளி கல்லூரிகளுக்கு விட...   \n",
       "3  தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது விஜய்க்க...   \n",
       "4  ஆழ்ந்த காற்றழுத்த தாழ்வுப்பகுதி எதிரொலி மாவட்ட...   \n",
       "5  உருவானது புதிய காற்றழுத்த தாழ்வுப்பகுதி எங்கெல...   \n",
       "6  பாமகவை இழுக்க முயற்சிக்கும் கறார் காட்டும் ராம...   \n",
       "7  வங்கக்கடலில் காற்றழுத்த தாழ்வுப்பகுதி தமிழகம் ...   \n",
       "8  அரபிக்கடலில் காற்றழுத்த தாழ்வு மண்டலம் நாளை மா...   \n",
       "9  அனிமேஷன் யுத்தம் இணையத்தில் மல்லுக்கட்டும் திம...   \n",
       "\n",
       "                                     tokenized_title  \\\n",
       "0  மேகதாது விவகாரம் தமிழக கர்நாடகா முதலமைச்சர்களு...   \n",
       "1  பந்துவீச்சாளர்கள் ஐபிஎல் விளையாடலாமா எதிரெதிர்...   \n",
       "2  கனமழை எச்சரிக்கை நாளை பள்ளி கல்லூரிகளுக்கு விட...   \n",
       "3  தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது விஜய்க்க...   \n",
       "4  ஆழ்ந்த காற்றழுத்த தாழ்வுப்பகுதி எதிரொலி மாவட்ட...   \n",
       "5  உருவானது புதிய காற்றழுத்த தாழ்வுப்பகுதி எங்கெல...   \n",
       "6  பாமகவை இழுக்க முயற்சிக்கும் கறார் காட்டும் ராம...   \n",
       "7  வங்கக்கடலில் காற்றழுத்த தாழ்வுப்பகுதி தமிழகம் ...   \n",
       "8  அரபிக்கடலில் காற்றழுத்த தாழ்வு மண்டலம் நாளை மா...   \n",
       "9  அனிமேஷன் யுத்தம் இணையத்தில் மல்லுக்கட்டும் திம...   \n",
       "\n",
       "                                     processed_title  \n",
       "0  மேகதாது விவகாரம் தமிழக கர்நாடகா முதலமைச்சர் நி...  \n",
       "1  பந்துவீச்சாளர் ஐபிஎல் விளையாடலாமா எதிரெதிர் கர...  \n",
       "2  கனமழை எச்சரிக்கை நாளை பள்ளி கல்லூரி விடுமுறை எ...  \n",
       "3  தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது விஜய் ஆர...  \n",
       "4  ஆழ் காற்றழு தாழ்வுப்பகுதி எதிரொலி மாவட்டங் அலர்ட்  \n",
       "5  உருவானது புதிய காற்றழு தாழ்வுப்பகுதி எங்கெல்லா...  \n",
       "6  பாமகவை இழுக்க முயற்சிக்கும் கறார் காட்டும் ராம...  \n",
       "7  வங்கக்கடலில் காற்றழு தாழ்வுப்பகுதி தமிழகம் முழ...  \n",
       "8  அரபிக்கடலில் காற்றழு தாழ்வு மண்டலம் நாளை மாவட்...  \n",
       "9  அனிமேஷன் யுத்தம் இணையத்தில் மல்லுக்கட்டும் திம...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"\\nProcessed Data Sample:\")\n",
    "processed_df[['title', 'cleaned_title', 'tokenized_title', 'processed_title']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e722f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['மேகதாது', 'விவகாரம்', 'தமிழக', 'கர்நாடகா', 'முதலமைச்சர்களுக்கு', 'நிதின்', 'கட்கரி', 'கடிதம்']\n",
      "1 ['பந்துவீச்சாளர்கள்', 'ஐபிஎல்', 'விளையாடலாமா', 'எதிரெதிர்', 'கருத்தில்', 'தோனி', 'கும்பளே']\n",
      "2 ['கனமழை', 'எச்சரிக்கை', 'நாளை', 'பள்ளி', 'கல்லூரிகளுக்கு', 'விடுமுறை', 'எங்கெல்லாம்', 'தெரியுமா']\n",
      "3 ['தவெகவை', 'ஆண்டவனாலும்', 'காப்பாற்ற', 'முடியாது', 'விஜய்க்கு', 'ஆர்பி', 'உதயகுமார்', 'அட்வைஸ்']\n",
      "4 ['ஆழ்ந்த', 'காற்றழுத்த', 'தாழ்வுப்பகுதி', 'எதிரொலி', 'மாவட்டங்களுக்கு', 'அலர்ட்']\n",
      "5 ['உருவானது', 'புதிய', 'காற்றழுத்த', 'தாழ்வுப்பகுதி', 'எங்கெல்லாம்', 'மழைக்கு', 'வாய்ப்பு']\n",
      "6 ['பாமகவை', 'இழுக்க', 'முயற்சிக்கும்', 'கறார்', 'காட்டும்', 'ராமதாஸ்', 'அன்புமணி', 'வைக்கும்', 'கோரிக்கை', 'என்ன', 'நடக்கிறது']\n",
      "7 ['வங்கக்கடலில்', 'காற்றழுத்த', 'தாழ்வுப்பகுதி', 'தமிழகம்', 'முழுவதும்', 'பரவலாக', 'மழை']\n",
      "8 ['அரபிக்கடலில்', 'காற்றழுத்த', 'தாழ்வு', 'மண்டலம்', 'நாளை', 'மாவட்டங்களில்', 'கனமழைக்கு', 'வாய்ப்பு']\n",
      "9 ['அனிமேஷன்', 'யுத்தம்', 'இணையத்தில்', 'மல்லுக்கட்டும்', 'திமுக', 'அதிமுக', 'ஐடிவிங்', 'என்ன', 'நடக்கிறது']\n"
     ]
    }
   ],
   "source": [
    "processed_df['tokenized_title'] = processed_df['cleaned_title'].apply(\n",
    "    lambda x: tokenizer.word_tokenize(x) if isinstance(x, str) else []\n",
    ")\n",
    "for idx, tokens in processed_df['tokenized_title'].head(10).items():\n",
    "    print(f\"{idx} {tokens}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cc45b4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARISON: Original vs Processed\n",
      "================================================================================\n",
      "\n",
      "[Example 1]\n",
      "Original:  மேகதாது விவகாரம்: தமிழக, கர்நாடகா முதலமைச்சர்களுக்கு நிதின் கட்கரி கடிதம்\n",
      "Cleaned:   மேகதாது விவகாரம் தமிழக கர்நாடகா முதலமைச்சர்களுக்கு நிதின் கட்கரி கடிதம்\n",
      "Processed: மேகதாது விவகாரம் தமிழக கர்நாடகா முதலமைச்சர் நிதின் கட்கரி கடிதம்\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Example 2]\n",
      "Original:  பந்துவீச்சாளர்கள் ஐபிஎல் விளையாடலாமா? - எதிரெதிர் கருத்தில் தோனி-கும்பளே\n",
      "Cleaned:   பந்துவீச்சாளர்கள் ஐபிஎல் விளையாடலாமா எதிரெதிர் கருத்தில் தோனி கும்பளே\n",
      "Processed: பந்துவீச்சாளர் ஐபிஎல் விளையாடலாமா எதிரெதிர் கருத்தில் தோனி கும்பளே\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Example 3]\n",
      "Original:  கனமழை எச்சரிக்கை | நாளை பள்ளி, கல்லூரிகளுக்கு விடுமுறை.. எங்கெல்லாம் தெரியுமா?\n",
      "Cleaned:   கனமழை எச்சரிக்கை நாளை பள்ளி கல்லூரிகளுக்கு விடுமுறை எங்கெல்லாம் தெரியுமா\n",
      "Processed: கனமழை எச்சரிக்கை நாளை பள்ளி கல்லூரி விடுமுறை எங்கெல்லாம் தெரியுமா\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Example 4]\n",
      "Original:  தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது - விஜய்க்கு ஆர்பி உதயகுமார் அட்வைஸ்\n",
      "Cleaned:   தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது விஜய்க்கு ஆர்பி உதயகுமார் அட்வைஸ்\n",
      "Processed: தவெகவை ஆண்டவனாலும் காப்பாற்ற முடியாது விஜய் ஆர்பி உதயகுமார் அட்வைஸ்\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Example 5]\n",
      "Original:  ஆழ்ந்த காற்றழுத்த தாழ்வுப்பகுதி எதிரொலி.. 8 மாவட்டங்களுக்கு RED அலர்ட்..\n",
      "Cleaned:   ஆழ்ந்த காற்றழுத்த தாழ்வுப்பகுதி எதிரொலி மாவட்டங்களுக்கு அலர்ட்\n",
      "Processed: ஆழ் காற்றழு தாழ்வுப்பகுதி எதிரொலி மாவட்டங் அலர்ட்\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARISON: Original vs Processed\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx in range(min(5, len(processed_df))):\n",
    "    print(f\"\\n[Example {idx+1}]\")\n",
    "    print(f\"Original:  {processed_df.iloc[idx]['title']}\")\n",
    "    print(f\"Cleaned:   {processed_df.iloc[idx]['cleaned_title']}\")\n",
    "    print(f\"Processed: {processed_df.iloc[idx]['processed_title']}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8b14fb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token Count Statistics:\n",
      "count    18448.000000\n",
      "mean         8.062392\n",
      "std          2.352002\n",
      "min          1.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%         10.000000\n",
      "max         15.000000\n",
      "Name: token_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "processed_df['token_count'] = processed_df['processed_title'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"\\nToken Count Statistics:\")\n",
    "print(processed_df['token_count'].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1b6688e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Final dataset exported to: output/final_dataset.csv\n",
      "  Shape: (18448, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_df = processed_df[['category', 'processed_title']]  \n",
    "final_df.to_csv('output/final_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\n✓ Final dataset exported to: output/final_dataset.csv\")\n",
    "print(f\"  Shape: {final_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1ad8d5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "இந்தியா                2192\n",
      "உலகம்                  1894\n",
      "குற்றம்                1627\n",
      "தமிழ்நாடு              1591\n",
      "சினிமா                 1456\n",
      "டெக்                   1217\n",
      "வணிகம்                 1208\n",
      "சிறப்புக் களம்         1196\n",
      "LIVE UPDATES            900\n",
      "கிரிக்கெட்              895\n",
      "ஹெல்த்                  849\n",
      "சுற்றுச்சூழல்           639\n",
      "விவசாயம்                614\n",
      "T20                     459\n",
      "டிரெண்டிங்              374\n",
      "விளையாட்டு              356\n",
      "கோலிவுட் செய்திகள்      204\n",
      "ஆன்மீகம்                155\n",
      "திரை விமர்சனம்           70\n",
      "இலக்கியம்                59\n",
      "ஓடிடி திரைப் பார்வை      52\n",
      "கதைகள்                   49\n",
      "லைஃப்ஸ்டைல்              44\n",
      "மார்க்கெட்               42\n",
      "கோயில்கள்                38\n",
      "கொரோனா வைரஸ்             38\n",
      "பட்ஜெட் 2025             28\n",
      "தங்கம்                   28\n",
      "கார்                     28\n",
      "செஸ்                     21\n",
      "பெண்கள்                  20\n",
      "கல்வி                    18\n",
      "மோட்டார்                 17\n",
      "பாலிவுட் செய்திகள்       15\n",
      "கால்பந்து                12\n",
      "ஸ்மார்ட்ஃபோன்            11\n",
      "ஹாலிவுட் செய்திகள்       11\n",
      "ஜோதிடம்                   9\n",
      "வேலை வாய்ப்பு             6\n",
      "டென்னிஸ்                  5\n",
      "பிக்பாஸ்                  4\n",
      "வீடியோ ஸ்டோரி             1\n",
      "கபடி                      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['category'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
